<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>trelliscope R Package Documentation</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Le styles -->
    <link href="bootstrap/css/bootstrap.css" rel="stylesheet">
    <link href="css/pygment.css" rel="stylesheet">
    
    <style type="text/css">
      body {
        padding-top: 20px;
        padding-bottom: 40px;
      }

      /* Custom container */
      .container-narrow {
        margin: 0 auto;
        max-width: 900px;
      }
      .container-narrow > hr {
        margin: 15px 0 20px 0;
      }
      
      #next {
         font-size: 14px;
      }

      #previous {
         font-size: 14px;
      }

      .fref_title {
         border-bottom:1px solid #EEE;
      }

      .myHeader {
         font-family: 'Chivo', 'Helvetica Neue', Helvetica, Arial, serif; font-weight: 400;
         letter-spacing: -1px;
         font-size: 28px;
         line-height: 40px;
         color: #d14;
         text-shadow: 8px 2px 6 rgba(55, 55, 55, 0.5);
      }

/*      #main-content {
         margin-top: -15px;
      }
*/
    </style>
        <style type="text/css">
       pre .operator,
       pre .paren { color: #555555 }

       pre .literal {
         color: rgb(88, 72, 246); font-weight: bold;
       }
/*       pre .literal { color: #006699; font-weight: bold } */

       pre .number { color: #FF6600 }
/*       pre .comment { color: #0099FF; font-style: italic }*/
       pre .comment { color: #999; font-style: italic }
       pre .keyword { color: #006699; font-weight: bold }

       pre .identifier {
         color: rgb(0, 0, 0);
       }

       pre .string { color: #CC3300 }
    </style>

    <!-- R syntax highlighter -->
    <script type="text/javascript">
    var hljs=new function(){function m(p){return p.replace(/&/gm,"&amp;").replace(/</gm,"&lt;")}function f(r,q,p){return RegExp(q,"m"+(r.cI?"i":"")+(p?"g":""))}function b(r){for(var p=0;p<r.childNodes.length;p++){var q=r.childNodes[p];if(q.nodeName=="CODE"){return q}if(!(q.nodeType==3&&q.nodeValue.match(/\s+/))){break}}}function h(t,s){var p="";for(var r=0;r<t.childNodes.length;r++){if(t.childNodes[r].nodeType==3){var q=t.childNodes[r].nodeValue;if(s){q=q.replace(/\n/g,"")}p+=q}else{if(t.childNodes[r].nodeName=="BR"){p+="\n"}else{p+=h(t.childNodes[r])}}}if(/MSIE [678]/.test(navigator.userAgent)){p=p.replace(/\r/g,"\n")}return p}function a(s){var r=s.className.split(/\s+/);r=r.concat(s.parentNode.className.split(/\s+/));for(var q=0;q<r.length;q++){var p=r[q].replace(/^language-/,"");if(e[p]){return p}}}function c(q){var p=[];(function(s,t){for(var r=0;r<s.childNodes.length;r++){if(s.childNodes[r].nodeType==3){t+=s.childNodes[r].nodeValue.length}else{if(s.childNodes[r].nodeName=="BR"){t+=1}else{if(s.childNodes[r].nodeType==1){p.push({event:"start",offset:t,node:s.childNodes[r]});t=arguments.callee(s.childNodes[r],t);p.push({event:"stop",offset:t,node:s.childNodes[r]})}}}}return t})(q,0);return p}function k(y,w,x){var q=0;var z="";var s=[];function u(){if(y.length&&w.length){if(y[0].offset!=w[0].offset){return(y[0].offset<w[0].offset)?y:w}else{return w[0].event=="start"?y:w}}else{return y.length?y:w}}function t(D){var A="<"+D.nodeName.toLowerCase();for(var B=0;B<D.attributes.length;B++){var C=D.attributes[B];A+=" "+C.nodeName.toLowerCase();if(C.value!==undefined&&C.value!==false&&C.value!==null){A+='="'+m(C.value)+'"'}}return A+">"}while(y.length||w.length){var v=u().splice(0,1)[0];z+=m(x.substr(q,v.offset-q));q=v.offset;if(v.event=="start"){z+=t(v.node);s.push(v.node)}else{if(v.event=="stop"){var p,r=s.length;do{r--;p=s[r];z+=("</"+p.nodeName.toLowerCase()+">")}while(p!=v.node);s.splice(r,1);while(r<s.length){z+=t(s[r]);r++}}}}return z+m(x.substr(q))}function j(){function q(x,y,v){if(x.compiled){return}var u;var s=[];if(x.k){x.lR=f(y,x.l||hljs.IR,true);for(var w in x.k){if(!x.k.hasOwnProperty(w)){continue}if(x.k[w] instanceof Object){u=x.k[w]}else{u=x.k;w="keyword"}for(var r in u){if(!u.hasOwnProperty(r)){continue}x.k[r]=[w,u[r]];s.push(r)}}}if(!v){if(x.bWK){x.b="\\b("+s.join("|")+")\\s"}x.bR=f(y,x.b?x.b:"\\B|\\b");if(!x.e&&!x.eW){x.e="\\B|\\b"}if(x.e){x.eR=f(y,x.e)}}if(x.i){x.iR=f(y,x.i)}if(x.r===undefined){x.r=1}if(!x.c){x.c=[]}x.compiled=true;for(var t=0;t<x.c.length;t++){if(x.c[t]=="self"){x.c[t]=x}q(x.c[t],y,false)}if(x.starts){q(x.starts,y,false)}}for(var p in e){if(!e.hasOwnProperty(p)){continue}q(e[p].dM,e[p],true)}}function d(B,C){if(!j.called){j();j.called=true}function q(r,M){for(var L=0;L<M.c.length;L++){if((M.c[L].bR.exec(r)||[null])[0]==r){return M.c[L]}}}function v(L,r){if(D[L].e&&D[L].eR.test(r)){return 1}if(D[L].eW){var M=v(L-1,r);return M?M+1:0}return 0}function w(r,L){return L.i&&L.iR.test(r)}function K(N,O){var M=[];for(var L=0;L<N.c.length;L++){M.push(N.c[L].b)}var r=D.length-1;do{if(D[r].e){M.push(D[r].e)}r--}while(D[r+1].eW);if(N.i){M.push(N.i)}return f(O,M.join("|"),true)}function p(M,L){var N=D[D.length-1];if(!N.t){N.t=K(N,E)}N.t.lastIndex=L;var r=N.t.exec(M);return r?[M.substr(L,r.index-L),r[0],false]:[M.substr(L),"",true]}function z(N,r){var L=E.cI?r[0].toLowerCase():r[0];var M=N.k[L];if(M&&M instanceof Array){return M}return false}function F(L,P){L=m(L);if(!P.k){return L}var r="";var O=0;P.lR.lastIndex=0;var M=P.lR.exec(L);while(M){r+=L.substr(O,M.index-O);var N=z(P,M);if(N){x+=N[1];r+='<span class="'+N[0]+'">'+M[0]+"</span>"}else{r+=M[0]}O=P.lR.lastIndex;M=P.lR.exec(L)}return r+L.substr(O,L.length-O)}function J(L,M){if(M.sL&&e[M.sL]){var r=d(M.sL,L);x+=r.keyword_count;return r.value}else{return F(L,M)}}function I(M,r){var L=M.cN?'<span class="'+M.cN+'">':"";if(M.rB){y+=L;M.buffer=""}else{if(M.eB){y+=m(r)+L;M.buffer=""}else{y+=L;M.buffer=r}}D.push(M);A+=M.r}function G(N,M,Q){var R=D[D.length-1];if(Q){y+=J(R.buffer+N,R);return false}var P=q(M,R);if(P){y+=J(R.buffer+N,R);I(P,M);return P.rB}var L=v(D.length-1,M);if(L){var O=R.cN?"</span>":"";if(R.rE){y+=J(R.buffer+N,R)+O}else{if(R.eE){y+=J(R.buffer+N,R)+O+m(M)}else{y+=J(R.buffer+N+M,R)+O}}while(L>1){O=D[D.length-2].cN?"</span>":"";y+=O;L--;D.length--}var r=D[D.length-1];D.length--;D[D.length-1].buffer="";if(r.starts){I(r.starts,"")}return R.rE}if(w(M,R)){throw"Illegal"}}var E=e[B];var D=[E.dM];var A=0;var x=0;var y="";try{var s,u=0;E.dM.buffer="";do{s=p(C,u);var t=G(s[0],s[1],s[2]);u+=s[0].length;if(!t){u+=s[1].length}}while(!s[2]);if(D.length>1){throw"Illegal"}return{r:A,keyword_count:x,value:y}}catch(H){if(H=="Illegal"){return{r:0,keyword_count:0,value:m(C)}}else{throw H}}}function g(t){var p={keyword_count:0,r:0,value:m(t)};var r=p;for(var q in e){if(!e.hasOwnProperty(q)){continue}var s=d(q,t);s.language=q;if(s.keyword_count+s.r>r.keyword_count+r.r){r=s}if(s.keyword_count+s.r>p.keyword_count+p.r){r=p;p=s}}if(r.language){p.second_best=r}return p}function i(r,q,p){if(q){r=r.replace(/^((<[^>]+>|\t)+)/gm,function(t,w,v,u){return w.replace(/\t/g,q)})}if(p){r=r.replace(/\n/g,"<br>")}return r}function n(t,w,r){var x=h(t,r);var v=a(t);var y,s;if(v){y=d(v,x)}else{return}var q=c(t);if(q.length){s=document.createElement("pre");s.innerHTML=y.value;y.value=k(q,c(s),x)}y.value=i(y.value,w,r);var u=t.className;if(!u.match("(\\s|^)(language-)?"+v+"(\\s|$)")){u=u?(u+" "+v):v}if(/MSIE [678]/.test(navigator.userAgent)&&t.tagName=="CODE"&&t.parentNode.tagName=="PRE"){s=t.parentNode;var p=document.createElement("div");p.innerHTML="<pre><code>"+y.value+"</code></pre>";t=p.firstChild.firstChild;p.firstChild.cN=s.cN;s.parentNode.replaceChild(p.firstChild,s)}else{t.innerHTML=y.value}t.className=u;t.result={language:v,kw:y.keyword_count,re:y.r};if(y.second_best){t.second_best={language:y.second_best.language,kw:y.second_best.keyword_count,re:y.second_best.r}}}function o(){if(o.called){return}o.called=true;var r=document.getElementsByTagName("pre");for(var p=0;p<r.length;p++){var q=b(r[p]);if(q){n(q,hljs.tabReplace)}}}function l(){if(window.addEventListener){window.addEventListener("DOMContentLoaded",o,false);window.addEventListener("load",o,false)}else{if(window.attachEvent){window.attachEvent("onload",o)}else{window.onload=o}}}var e={};this.LANGUAGES=e;this.highlight=d;this.highlightAuto=g;this.fixMarkup=i;this.highlightBlock=n;this.initHighlighting=o;this.initHighlightingOnLoad=l;this.IR="[a-zA-Z][a-zA-Z0-9_]*";this.UIR="[a-zA-Z_][a-zA-Z0-9_]*";this.NR="\\b\\d+(\\.\\d+)?";this.CNR="\\b(0[xX][a-fA-F0-9]+|(\\d+(\\.\\d*)?|\\.\\d+)([eE][-+]?\\d+)?)";this.BNR="\\b(0b[01]+)";this.RSR="!|!=|!==|%|%=|&|&&|&=|\\*|\\*=|\\+|\\+=|,|\\.|-|-=|/|/=|:|;|<|<<|<<=|<=|=|==|===|>|>=|>>|>>=|>>>|>>>=|\\?|\\[|\\{|\\(|\\^|\\^=|\\||\\|=|\\|\\||~";this.ER="(?![\\s\\S])";this.BE={b:"\\\\.",r:0};this.ASM={cN:"string",b:"'",e:"'",i:"\\n",c:[this.BE],r:0};this.QSM={cN:"string",b:'"',e:'"',i:"\\n",c:[this.BE],r:0};this.CLCM={cN:"comment",b:"//",e:"$"};this.CBLCLM={cN:"comment",b:"/\\*",e:"\\*/"};this.HCM={cN:"comment",b:"#",e:"$"};this.NM={cN:"number",b:this.NR,r:0};this.CNM={cN:"number",b:this.CNR,r:0};this.BNM={cN:"number",b:this.BNR,r:0};this.inherit=function(r,s){var p={};for(var q in r){p[q]=r[q]}if(s){for(var q in s){p[q]=s[q]}}return p}}();hljs.LANGUAGES.r={dM:{c:[hljs.HCM,{cN:"number",b:"\\b0[xX][0-9a-fA-F]+[Li]?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+(?:[eE][+\\-]?\\d*)?L\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\b\\d+\\.(?!\\d)(?:i\\b)?",e:hljs.IMMEDIATE_RE,r:1},{cN:"number",b:"\\b\\d+(?:\\.\\d*)?(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"number",b:"\\.\\d+(?:[eE][+\\-]?\\d*)?i?\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"keyword",b:"(?:tryCatch|library|setGeneric|setGroupGeneric)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\.",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\.\\.\\d+(?![\\w.])",e:hljs.IMMEDIATE_RE,r:10},{cN:"keyword",b:"\\b(?:function)",e:hljs.IMMEDIATE_RE,r:2},{cN:"keyword",b:"(?:if|in|break|next|repeat|else|for|return|switch|while|try|stop|warning|require|attach|detach|source|setMethod|setClass)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"literal",b:"(?:NA|NA_integer_|NA_real_|NA_character_|NA_complex_)\\b",e:hljs.IMMEDIATE_RE,r:10},{cN:"literal",b:"(?:NULL|TRUE|FALSE|T|F|Inf|NaN)\\b",e:hljs.IMMEDIATE_RE,r:1},{cN:"identifier",b:"[a-zA-Z.][a-zA-Z0-9._]*\\b",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"<\\-(?!\\s*\\d)",e:hljs.IMMEDIATE_RE,r:2},{cN:"operator",b:"\\->|<\\-",e:hljs.IMMEDIATE_RE,r:1},{cN:"operator",b:"%%|~",e:hljs.IMMEDIATE_RE},{cN:"operator",b:">=|<=|==|!=|\\|\\||&&|=|\\+|\\-|\\*|/|\\^|>|<|!|&|\\||\\$|:",e:hljs.IMMEDIATE_RE,r:0},{cN:"operator",b:"%",e:"%",i:"\\n",r:1},{cN:"identifier",b:"`",e:"`",r:0},{cN:"string",b:'"',e:'"',c:[hljs.BE],r:0},{cN:"string",b:"'",e:"'",c:[hljs.BE],r:0},{cN:"paren",b:"[[({\\])}]",e:hljs.IMMEDIATE_RE,r:0}]}};
    hljs.initHighlightingOnLoad();
    </script>

    <!-- MathJax scripts -->
    <script type="text/javascript" src="js/MathJax/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    MathJax.Hub.Config({    
      extensions: ["tex2jax.js"],    
      "HTML-CSS": { scale: 100}    
    });    
    </script>
    <link href="bootstrap/css/bootstrap-responsive.css" rel="stylesheet">

    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="ico/apple-touch-icon-114-precomposed.png">
      <link rel="apple-touch-icon-precomposed" sizes="72x72" href="ico/apple-touch-icon-72-precomposed.png">
                    <link rel="apple-touch-icon-precomposed" href="ico/apple-touch-icon-57-precomposed.png">
                                   <!-- <link rel="shortcut icon" href="ico/favicon.png"> -->
  </head>

  <body>

    <div class="container-narrow">

      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li class="active"><a href="index.html">Docs</a></li>
          <li class=""><a href="functionref.html">Function Ref</a></li>
          <li><a href="https://www.github.com/hafen/trelliscope">Github</a></li>
        </ul>
        <p class="myHeader">Trelliscope: Detailed Vis of Large Complex Data in R</p>
      </div>

      <hr>

<div class="container-fluid">
   <div class="row-fluid">
   
   <div class="span3 well">
   <ul class = "nav nav-list" id="toc">
   <li class='nav-header'>Intro</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#background'>Background</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#initial-setup'>Initial Setup</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#splitting-data'>Splitting Data</a>
      </li>


<li class='nav-header'>Creating Displays</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#a-bare-bones-display'>A Bare Bones Display</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#the-prepanel-function'>The Prepanel Function</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#determining-axis-limits'>Determining Axis Limits</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#cognostics'>Cognostics</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#putting-it-all-together'>Putting it All Together</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#another-display'>Another Display</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#single-panel-displays'>Single-Panel Displays</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#splod'>"splod"</a>
      </li>


<li class='nav-header'>Viewing Displays</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#shiny-viewer'>Shiny Viewer</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#cognostics-interactions'>Cognostics Interactions</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#client-side-viewer'>Client-Side Viewer</a>
      </li>


<li class='nav-header'>Scalable Setup</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#architecture'>Architecture</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#hadoop-r-rhipe'>Hadoop, R, RHIPE</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#other-components'>Other Components</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#vdb-connections'>VDB Connections</a>
      </li>


<li class='nav-header'>RHIPE</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#trelliscope-with-rhipe'>Trelliscope with RHIPE</a>
      </li>


<li class='nav-header'>Notes on Storage</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#panel-storage-types'>Panel Storage Types</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#cognostics-storage-types'>Cognostics Storage Types</a>
      </li>


<li class='nav-header'>Lab Notebooks</li>
      
      <li class='active'>
         <a target='_self' class='nav-not-header' href='#creating-a-notebook'>Creating a Notebook</a>
      </li>


      <li class='active'>
         <a target='_self' class='nav-not-header' href='#syncing-files-with-the-web'>Syncing Files With the Web</a>
      </li>

   </ul>
   </div>

<div class="span9 tab-content" id="main-content">

<div class='tab-pane active' id='background'>
<h3>Background</h3>

<p>This documentation provides examples of creating and viewing visualization databases (VDBs) using the <code>trelliscope</code> R package.  Trelliscope provides a way to visualize large, complex data in great detail from within the R statistical programming environment.  This package operates on data that has been split into subsets using the <a href="http://github.com/hafen/datadr">datadr</a> package.  Data can be local R data.frames or can be very massive <a href="http://github.com/saptarshiguha/RHIPE">RHIPE</a> datasets.  This is a research effort that is evolving, and this document communicates the current state.</p>

<p>Trelliscope is based on the concept of Divide and Recombine (D&amp;R).  Some philosophy and background on D&amp;R can be found at <a href="http://www.datadr.org">datadr.org</a>.</p>

<h4>Motivation</h4>

<p>Building statistical models that adequately explain the patterns in the data is an iterative process, and visualization plays a critical role throughout.  In the initial exploration phases, visualization is one of the most powerful mechanisms through which the analyst can begin to formulate and hypothesize mathematical models that represent the patterns in the data.  After applying algorithms and fitting models, visualization plays a key role in validating the adequacy of the model.  Finally, after iterating and arriving upon an adequate model, visualization plays a critical role in disseminating the results of the analytical task.  To ensure information is not lost, the ability to visualize the data at the finest level of detail is crucial at all steps in this process, regardless of the size of the data.</p>

<h4>The Visualization Context: Data Analysis</h4>

<p>An important point to get across is who the consumer of the visualization is in this process.  Many visualization tools are designed from the point of view that the end user is a field analyst, with little or no training in statistical modeling.  The algorithms for the data have already been specified, and the purpose of the visualization tool is to assist the field analyst in understanding the data based on these algorithms.  These types of visualization tools can be very useful for their intended purpose, but there is a need for scalable visualization tools that can be used during the statistical model building process.</p>

<!-- Tools of this nature that we have had experience with can be very useful, but are not appropriate for our analysis context.   -->

<!-- but are usually specialized or take a long time to develop for a specific dataset. -->

<p>The context for our visualization environment is <em>deep statistical analysis and machine learning</em>, building and validating mathematical models that capture the behavior in the data.  The consumer of visualization tools in this context is not a field analyst but a statistician with in-depth knowledge about a wealth of statistical methods, the assumptions these methods make about the data, and with experience in fitting and validating models.  In this context, the analyst needs tools that are very flexible that allow for rapid prototyping.  Most importantly, visualization tools in this context need to tie directly into the data analysis environment, as visual and numerical methods cannot be decoupled.</p>

<!-- But where do the algorithms used by these tools come from?  In many cases they come from the iterative statistical model building process we defined previously.  In this process, the visualization consumer is a person with in-depth knowledge about a wealth of statistical methods, the assumptions these methods make about the data, and with experience in fitting and validating models.  Thus the end-user of the visualization tools we are discussing is very different from what is traditionally thought of.  This is an important distinction to keep in mind. -->

<!-- It is important to make sure the action of "visualization" is being thought of in the appropriate context.  The context here is *deep statistical analysis*.  The consumer of these tools is not a field analyst but a statistician who is trying to use visualization to hypothesize and validate models that account for variability in the data.  As such, we are not as interested in tools built for a specific type of data or a specific domain.  Such tools can be very useful, particularly if they are based on statistical models that have been validated against the data, but in our setting, we are actually *building* and *validating* the models.   Thus, we need a flexible tool with broad applicability, that ties directly into our data analysis environment. -->

<h4>General Approach: Dividing the Data</h4>

<p>An effective approach to detailed data visualization with many records or variables is to divide the data into meaningful subsets and apply a visualization method to each subset, creating a collection of <a href="http://www.edwardtufte.com/tufte/books_ei">&quot;small multiples&quot;</a>.  Viewing a collection of small multiples is effective in revealing repetitions or changes, and allows an analyst to see the data at finer levels of detail.  This visualization approach has been an effective tool for many years and is built in to many successful visualization software packages, such as the lattice package for the R statistical computing environment based on <a href="http://www.cs.ubc.ca/%7Etmm/courses/infovis/readings/trellis.jstor.pdf">trellis</a> display for S/S-plus.  The analyst creates a visual display, applying a visualization method across subsets of the data, resulting in a collection of small multiples, each of which is called a panel in the display.  Many displays are created throughout the course of an analysis. A collection of displays is called a visualization database <a href="http://jmlr.csail.mit.edu/proceedings/papers/v5/guha09a/guha09a.pdf">(VDB)</a>.</p>

<h4>Scaling to Massive Data</h4>

<p>As the size or complexity of the data increases, the number of small multiples increases beyond the capacity of the human mind to simultaneously capture the details and larger scale trends or patterns.  A <a href="http://www.jstor.org/discover/10.2307/1558733?uid=3739960&amp;uid=2129&amp;uid=2&amp;uid=70&amp;uid=4&amp;uid=3739256&amp;sid=21102145454567">simple idea</a> put forth by John Tukey decades ago is to have the computer do the work of determining which displays might be interesting.  This is accomplished through calculating diagnostic quantities for each panel to provide a rank for its potential usefulness.  Tukey called these quantities computer guided diagnostics, or <a href="http://books.google.com/books?id=z_F5Tyt66c0C&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false">cognostics</a>.  Cognostic quantities can be simple statistical quantities (range, standard deviation, model coefficients) or can be tailored to the visualization at hand to differentiate among large numbers of panels with respect to context-relevant attributes.  Panels in a display can be sorted and filtered based on cognostics of interest, or samples of panels stratified according to the distribution of various cognostics.</p>

<h4>Design of a Detailed Visualization Environment for Statistical Model Building</h4>

<p>Summarizing some points above, visualization in the statistical model building process requires an environment that:</p>

<ul>
<li>provides rapid development of visualizations</li>
<li>provides a great degree of flexibility</li>
<li>ties directly into our data analysis environment</li>
</ul>

<p>To address these requirements, our visualization environment is built upon R.  R satisfies all 3 of these requirements.  While there are many excellent visualization frameworks we could make use of, one of the driving forces for this choice is the tie-in with all of the thousands of numerical methods available in R, and that R vis packages such as lattice, ggplot, and base R are familiar to statisticians.  As it becomes easier to rapidly build visualizations using more modern web-based vis tools such as vega/d3 from R, we will start to consider these.  We are already investigating the use of such frameworks for the interactive panel viewer part of this environment.</p>

<!-- #### The trelliscope Package -->

<!-- In short, a visualization database is a collection of displays, each of which visualizes some aspect of the data being analyzed.  A display consists of one or more panels, where a panel is a plot method applied to each subset of the data.   -->

<!-- The `trelliscope` package provides mechanisms which, after using the [datadr][datadr] package to divide the data, allow you to apply visualization methods to the divided data and then interactively view the panels of the display in meaningful ways, based on cognostics computed for each panel that quantify interesting features.  This package also provides some mechanisms for maintaining a "lab notebook" with hooks for injecting your displays into the notebook.  It is also useful for storing and organizing single-panel plots.  When used with very large [RHIPE][rhipe] datasets, it allows interactive detailed visualization of large, complex data, and has been tested on displays with hundreds of thousands of panels per display. -->

<!-- Suppose, for example, I have a large collection of high energy physics sensor data, and I am trying to study the properties of ...  I need to rapidly change between numerical and visual methods throughout the process of model building. -->

<h4>Reference</h4>

<p>Related projects:</p>

<ul>
<li><a href="http://github.com/saptarshiguha/RHIPE">RHIPE</a>: the engine that makes Trelliscope work with large, complex data</li>
<li><a href="http://github.com/hafen/datadr">datadr</a>: division methods</li>
</ul>

<p>References:</p>

<ul>
<li><a href="http://datadr.org">http://datadr.org</a></li>
<li><a href="http://onlinelibrary.wiley.com/doi/10.1002/sta4.7/full">Large complex data: divide and recombine (D&amp;R) with RHIPE. <em>Stat</em>, 1(1), 53-67</a></li>
<li><a href="http://jmlr.csail.mit.edu/proceedings/papers/v5/guha09a/guha09a.pdf">Visualization Databases for the Analysis of Large Complex Datasets</a></li>
</ul>

</div>


<div class='tab-pane' id='initial-setup'>
<h3>Initial Setup</h3>

<p>Trelliscope has a project-oriented nature.  It is expected that you will usually create a new VDB of Trelliscope displays for a new project.  Within a project, there may be many analysis threads, and Trelliscope allows for displays to be grouped.  All files for a VDB will reside in a subdirectory inside your project directory, which you can specify (it&#39;s recommended to use an absolute path).</p>

<pre><code class="r">library(datadr)
library(trelliscope)

# initialize a VDB (do this once per project)
vdbDir &lt;- &quot;/tmp/vdbtest&quot;
</code></pre>

<p>Now, we need to initialize some files in this directory.  This again is a once-per-project task.</p>

<pre><code class="r"># move files into place
vdbInit(vdbDir, name=&quot;testVDB&quot;, autoYes=TRUE)
# don&#39;t specify autoYes generally
</code></pre>

<pre><code>Moving viewer files over
Moving notebook files over
</code></pre>

<p>To see what happened:</p>

<pre><code class="r">list.files(vdbDir)
</code></pre>

<pre><code>[1] &quot;conn.R&quot;  &quot;displays&quot;  &quot;notebook&quot;  &quot;trelliscopeViewer_cs&quot;
</code></pre>

<p>Three directories exist in our VDB base directory, as well as a file, <code>conn.R</code>.  The <code>display</code> directory is where images and meta data related to our trelliscope displays are stored.  The <code>notebook</code> directory is where web reports of your project analysis can go and the <code>trelliscopeViewer_cs</code> directory contains files used for a client-side viewer for your VDB displays (more on both of these later).  The <code>conn.R</code> file is a template configuration file for your VDB connection, populated with the directory and name you specified.  Other lines with examples of other available parameters are also visible in the file which help your VDB know how to communicate with other possible services (such as a Hadoop server, MongoDB, web server).  For simple usage, these are not necessary, and we discuss them in greater detail later on.  </p>

<p><code>vdbInit()</code> reads the <code>conn.R</code> file and sets an option in the environment with these settings.  You can see what is set with:</p>

<pre><code class="r"> getOption(&quot;vdbConn&quot;)
</code></pre>

<pre><code>$vdbName
[1] &quot;testVDB&quot;

$vdbPrefix
[1] &quot;/tmp/vdbTest&quot;

$defaultStorage
[1] &quot;local&quot;
</code></pre>

<p>These options tell future <code>trelliscope</code> function calls that the current VDB is &quot;testVDB&quot; and its files are located in &quot;/tmp/vdbTest&quot;, and that the default storage method is &quot;local&quot; (more on storage methods later).  We will also discuss other <code>vdbConn</code> options later on.</p>

<p>The reason for setting a <code>vdbConn</code> list of options is that these options are fairly routinely required for functions in the <code>trelliscope</code> package and do not change over the course of an R session and are quite tedious to have to specify every time.</p>

<p>In all subsequent sessions once a VDB has been initialized, you can load the VDB connection settings using:</p>

<pre><code class="r"># new R session
library(trelliscope)
vdbConnect(&quot;/tmp/vdbTest&quot;)
</code></pre>

<p>This will locate and load the <code>conn.R</code> file.</p>

<p>We are now ready to start creating some displays, once we get data in a suitable format.</p>

</div>


<div class='tab-pane' id='splitting-data'>
<h3>Splitting Data</h3>

<p>I have included a test dataset in this package called <code>elnino</code>.  It consists of a time series of several variables measured by several buoys located across the Pacific ocean.</p>

<!-- 
data(elnino)
data(elninoMap)
xtick <- seq(-50, 100, by=10)
ytick <- seq(-30, 30, by=10)
xyplot(lat ~ lon, groups=buoy, data=elnino,
   panel=function(x, y, ...) {
      panel.fill("gray")
      panel.abline(v=xtick, col="lightgray")
      panel.abline(h=ytick, col="lightgray")
      panel.superpose(x, y, ...)
      panel.lines(elninoMap$x, elninoMap$y, col="white")      
   },
   panel.groups=function(x, y, ...) {
      xmean <- mean(x, na.rm=TRUE)
      ymean <- mean(y, na.rm=TRUE)
      panel.points(xmean, ymean, cex=1.1)
   },
   ylim=c(-25, 25),
   xlim=c(-60, 110),
   pch=".",
   scales=list(x=list(at=xtick), y=list(at=ytick)),
   aspect="iso",
   xlab="Longitude", ylab="Latitude"
)
-->

<p>This map is the average location of each buoy.</p>

<p><center>
   <img src="figs/1plot.png" width="650px">
</center></p>

<p>Time series of several measurements are available at each location:</p>

<pre><code class="r">data(elnino)
head(elnino)
</code></pre>

<pre><code>       buoy       date   lat   lon zonWinds merWinds humidity airTemp ssTemp
1 buoyid_27 1980-03-07 -0.02 70.54     -6.8      0.7       NA   26.14  26.24
2 buoyid_27 1980-03-08 -0.02 70.54     -4.9      1.1       NA   25.66  25.97
3 buoyid_27 1980-03-09 -0.02 70.54     -4.5      2.2       NA   25.69  25.28
4 buoyid_27 1980-03-10 -0.02 70.54     -3.8      1.9       NA   25.57  24.31
5 buoyid_27 1980-03-11 -0.02 70.54     -4.2      1.5       NA   25.30  23.19
6 buoyid_27 1980-03-12 -0.02 70.54     -4.4      0.3       NA   24.72  23.64
</code></pre>

<p>We see that a buoy ID, date, geolocation, and several weather-related variables are given.  The buoys for the most part stay within the vicinity of a fixed location, but some drift quite a bit.  The buoy categorization was determined by binning the geolocations, and some categorizations are suspect, as we will see.</p>

<p>An important aspect of plotting with <code>trelliscope</code> is the notion of working with data that has been divided into subsets.  This can be done in many ways, and we use the <a href="http://github.com/hafen/datadr">datadr</a> package to do so.  The <code>datadr</code> package has facilities for dividing both in-memory data.frames and very large <a href="http://github.com/saptarshiguha/RHIPE">RHIPE</a> data objects.  For illustrative purposes, and for easy entry, we stick to the in-memory <code>elnino</code> data for this document, but do keep in mind that this all works for (and is primarily intended for) large data as well.</p>

<p>There are many ways we might want to split the data, depending on the purpose of our analysis.  Suppose we are interested in viewing the time series of <code>elnino</code> data by buoy.</p>

<pre><code class="r">buoySplit &lt;- divide(elnino, by=&quot;buoy&quot;)
</code></pre>

<p>The <code>divide</code> function takes a data.frame as an argument and splits it by the unique levels one or more conditioning variable.  The result is a list of class &quot;localDiv&quot;:</p>

<pre><code class="r">buoySplit
</code></pre>

<pre><code>An object of class localDiv with the following attributes: 

&#39;localDiv&#39; attr : value
--------------------------------------------------------------------------------
* vars          : buoy(cha), date(Dat), lat(num), lon(num), zonWinds(num), merWinds(num), humidity(num), airTemp(num), ssTemp(num)
* totSize       : 13.04 MB
* ndiv          : 67
* trans         : identity transformation (original data is a data.frame)
* nrow          : 178080
* splitRowDistn : use dat$splitRowDistn to get distribution

First-order division:
  Type: Conditioning variable division
    Conditioning variables: buoy
</code></pre>

<p>There are 67 subsets, one for each buoy.  We can see what the data structure looks like:</p>

<pre><code class="r">str(buoySplit[[1]])
# or
str(divExample(buoySplit))
</code></pre>

<pre><code>&#39;data.frame&#39;:   2090 obs. of  9 variables:
 $ buoy    : chr  &quot;buoyid_01&quot; &quot;buoyid_01&quot; &quot;buoyid_01&quot; &quot;buoyid_01&quot; ...
 $ date    : Date, format: &quot;1992-09-22&quot; &quot;1992-09-23&quot; ...
 $ lat     : num  -7.97 -7.97 -7.97 -7.97 -7.97 -7.97 -7.97 -7.97 -7.97 -7.99 ...
 $ lon     : num  55 55 55 55 55 ...
 $ zonWinds: num  -4.6 -4.5 -5.4 -4.8 -5.9 -7 -6.6 -6.6 -7.6 -6.6 ...
 $ merWinds: num  -0.2 -2.1 -1.3 -0.6 -0.3 -0.6 0.1 1.8 0.2 -0.4 ...
 $ humidity: num  NA NA NA NA NA NA NA NA NA NA ...
 $ airTemp : num  NA NA NA NA NA NA NA NA NA NA ...
 $ ssTemp  : num  NA 26.4 26.4 26.3 26.3 ...
</code></pre>

<p>The convenience function <code>divExample</code> grabs the first subset.  This is a useful function as it operates on RHIPE split data as well and is useful for testing plotting functions.  Notice that <code>buoySplit</code> is simply a list of subset data.frames.</p>

<p>Now we are ready to make some plots.</p>

</div>


<div class='tab-pane' id='a-bare-bones-display'>
<h3>A Bare Bones Display</h3>

<p>To quickly get our feet wet with creating a display, we start with a minimal example.  Creating a plot first requires the specification of what you would like to be plotted.  You create a plot function of what you would like to visualize for each subset.  The function takes one argument, which you can think of as an object containing any one subset of your data.  Some things to know about the plot function:</p>

<ul>
<li>The plot function is applied to each subset of your split data object</li>
<li>The plot function takes one input, which is one of these subsets</li>
<li>The plot function returns something that can be printed to a graphics device</li>
<li>Those familiar with lattice can think of the plot function as the lattice panel function and the data argument as the lattice packet being plotted (except that you conveniently get the whole data structure instead of just <code>x</code> and <code>y</code>)</li>
<li>As long as the plotting method you use can produce graphics in a graphics device, you can use it: base R graphics, lattice, or ggplot2 are all available </li>
<li>However, using something like lattice or ggplot2 adds benefit because these create objects which can be inspected to pull out axis limits, etc. (see our discussion of prepanel functions later on)</li>
<li>If you use something that doesn&#39;t create an object (like base R graphics), you need to wrap the list of plot commands in an expression as the return value.</li>
</ul>

<p>Suppose we want to do a simple time series plot that highlights points where there are duplicated values for a single date.  With lattice, we can do this with the following:</p>

<pre><code class="r"># start with something simple...
airDatePlotFn &lt;- function(dat) {
   xyplot(airTemp ~ date, 
      groups=duplicated(date) | duplicated(date, fromLast=TRUE), 
      data=dat
   )
}
</code></pre>

<p>As mentioned, the argument <code>dat</code> (you can call it whatever you like) will have the data structure of any single split of your data.  Simply, we are plotting <code>airTemp</code> vs. <code>date</code>, and these variables reside in the data.frame of the current split being plotted, <code>dat</code>.</p>

<p>When developing the plot function, it can be useful to build it up based on an example subset:</p>

<pre><code class="r">dat &lt;- divExample(buoySplit)
</code></pre>

<p>The <code>groups=</code> line in the plot function basically creates a boolean that is <code>TRUE</code> if any of the time points have more than one value, which will be plotted with a different color.</p>

<p>To test out this function on a subset:</p>

<p><center>
   <img src="figs/2plot1.png" width="500px">
</center></p>

<pre><code class="r">airDatePlotFn(divExample(buoySplit))
</code></pre>

<p>Now, let&#39;s create a display in our visualization database based on this plot function:</p>

<pre><code class="r">makeDisplay(
   name    = &quot;airTemp_vs_date_plain&quot;,
   group   = &quot;buoy&quot;,
   data    = buoySplit,
   plotFn  = airDatePlotFn
)
</code></pre>

<pre><code>Validating &#39;plotFn&#39;...
Validating plot dimensions...
Precomputed limits not supplied.  Computing axis limits...
... skipping this step since both are axes are free ...
Generating plots...
-- Plotting panel 67 of 67 
Updating displayList...
Updating displayList...
Writing panel keys...
Writing cognostics...
Plotting thumbnail...
</code></pre>

<p>This example shows the minimum number of parameters that must be specified to create a display: we must specify a <code>name</code> and <code>group</code> for the display, the split <code>data</code> it will be based on, and the plot function, <code>plotFn</code>.  Note that this will work just the same with RHIPE split data.</p>

<p>The name and group collectively uniquely define a display and determine where the display is stored.  The group parameter helps keep track of displays by group, as an analysis may have many different threads.  We chose &quot;buoy&quot; as the group, as this display is related to an analysis of the data split by buoys.  By default, <code>group=&quot;common&quot;</code> (so it really doesn&#39;t need to be specified either if you are happy with that).</p>

<p>The default behavior for how plots are stored when the input is a local dataset is to store a .png file for each subset on disk.  I am experimenting with many storage formats (including applying the plot function on-the-fly in the viewer), which is discussed later.</p>

<p>We now have files in our &quot;displays&quot; directory:</p>

<pre><code class="r">list.files(file.path(vdbDir, &quot;displays&quot;))
</code></pre>

<pre><code>[1] &quot;_displayList.json&quot;  &quot;_displayList.Rdata&quot; &quot;buoy&quot;
</code></pre>

<p>The first two files beginning with <code>_displayList</code> are R and json versions of a master display list with different metadata about each display.  The third directory, &quot;buoy&quot; corresponds to the group that our newly created display belongs to, &quot;buoy&quot;.  Any display in the group &quot;buoy&quot; will be put here.</p>

<pre><code class="r">list.files(file.path(vdbDir, &quot;displays&quot;, &quot;buoy&quot;))
</code></pre>

<pre><code>[1] &quot;airTemp_vs_date_plain&quot;
</code></pre>

<p>We see our display, <code>airTemp_vs_date_plain</code> in here.  Let&#39;s see what is in there:</p>

<pre><code class="r">list.files(file.path(vdbDir, &quot;displays&quot;, &quot;buoy&quot;, &quot;airTemp_vs_date_plain&quot;))
</code></pre>

<pre><code>[1] &quot;cog.Rdata&quot;       &quot;json&quot;            &quot;object.Rdata&quot;    &quot;panelKeys.Rdata&quot;
[5] &quot;png&quot;             &quot;thumb.png&quot;
</code></pre>

<p>We see several files, most of which do not need to concern the user.  However, the <code>png</code> directory is where the actual plots are stored.  The png files are named according to their split key.  It is good to know these are here if you want to pull one out at random.  However, in general, you want to get at the actual panels of the display through the viewer.</p>

<p>To use the viewer, just type any of the following:</p>

<pre><code class="r">view()
view(group=&quot;buoy&quot;, name=&quot;airTemp_vs_date_plain&quot;)
view(name=&quot;airTemp_vs_date_plain&quot;)
</code></pre>

<p>This will bring up the web-based viewer of your display.  If you do plain <code>view()</code>, you will get a list of displays to choose from (at this point, just one).  The other two lines take you directly to the display we just created.  Notice that the last one works, even though I said that a display is uniquely identified by the group and name combination.  Usually, name is unique too and if that is the case, group is inferred.</p>

<p>Note that the aspect ratio is not ideal for this plot.  We&#39;ll show how to change this in the next example.</p>

<p>We&#39;ll also talk more about how to use the viewer later.</p>

<p>Now, let&#39;s do something that shows all of the other functionality of <code>makeDisplay</code>, including prepanel functions, cognostics, etc.</p>

</div>


<div class='tab-pane' id='the-prepanel-function'>
<h3>The Prepanel Function</h3>

<p>Usually prior to plotting you want to determine axis limits for each panel.  This is important because when viewing many small multiples, axis limits play a major role in making meaningful comparisons between panels.  Also, if a handful of panels have large outlying values, you might want to trim these so that you can view the bulk of the data in the panel at better resolution.  </p>

<p>Note that if you want to skip this extra step of code, you can specify <code>lims</code> in <code>makeDisplay</code> (or explicitly in your panel function), which will automatically perform the axis limit computations prior to creating the display (as we did in our previous example).</p>

<p>The <code>prepanel</code> function is the first step in determining axis limits.  For those familiar with aspect ratio banking, it also has preliminary support for calculating aspect ratios through banking.</p>

<p>The main parameter to know about in the <code>prepanel</code> function is <code>preFn</code>, which is a function that takes a subset of your split data as an input (like <code>plotFn</code> in <code>makeDisplay</code>) and returns a list including the elements <code>xlim</code> and <code>ylim</code> (each a vector of the min and max x and y ranges of the data subset), and optionally <code>dx</code> and <code>dy</code>, vectors used to determine the aspect ration when banking on 45 degrees.</p>

<p>If you are familiar with prepanel functions in the lattice package, you will see the similarity.  A prepanel function might sometimes be somewhat unrelated to what is actually plotted, but usually axis limits depend exactly on what is being plotted.  In this case, if your <code>plotFn</code> is a trellis object, you can simply specify your <code>plotFn</code> as <code>preFn</code>, for convenience.  The <code>prepanel</code> function will simply apply the plot and grab the axis limits from the object created (I suppose I could add this functionality for ggplot2 as well).  Note that in this case, however, you cannot specify <code>dx</code> and <code>dy</code>.</p>

</div>


<div class='tab-pane' id='determining-axis-limits'>
<h3>Determining Axis Limits</h3>

<p>We can now determine our axis limits based on the results from <code>prepanel</code>.  The output from <code>prepanel</code> is object of class &quot;trsPre&quot; which has a plot method that can help in the decision of how to specify limits.  Before we look at this plot, let&#39;s talk about the possible choices of limit specification (those familiar with lattice will know what these are):</p>

<ul>
<li><strong>&quot;same&quot;</strong>: the same limits are used for all the panels</li>
<li><strong>&quot;free&quot;</strong>: the limits for each panel are determined by just the points in that panel</li>
<li><strong>&quot;sliced&quot;</strong>:  the length (max - min) of the scales are constrained to remain the same across panels</li>
</ul>

<p>Determining suitable axis limits is dependent on what is being visualized, but typically &quot;same&quot; or &quot;sliced&quot; are good choices as they enable panel-to-panel comparisons, which is where much of the power of this type of visualization lies.  You might choose &quot;sliced&quot; if you are interested in relative behaviors in terms of scale, or &quot;same&quot; if you are interested in relative behaviors both in terms of location and scale.</p>

<p>To view a plot of the panel axis limits to help in this determination:</p>

<pre><code class="r">airDatePre &lt;- prepanel(buoySplit, airDatePlotFn)
plot(airDatePre)
</code></pre>

<p><center>
   <img src="figs/2plot2.png" width="650px">
</center></p>

<p>This plot orders the axis limits for both the x and y axes for both &quot;same&quot; and &quot;sliced&quot; (with sliced ranges centered around zero).  This can help us to see if we will be squeezing the data for a lot of panels when using &quot;same&quot;, and also helps identify outliers.  In each of the panels of this plot, you can think of the range of the &quot;Panel Limits&quot; axis as the range that will ultimately be chosen for each panel for the given axis and limit method.</p>

<p>For this plot, it appears that slicing the axis limits does not buy us much resolution, and we know that for the time variable axis (x), we would like the limits to always be the same.  Thus, we choose &quot;same&quot; for both axes.</p>

<p>To set our choice, we do the following:</p>

<pre><code class="r">buoyLims &lt;- setLims(airDatePre) # default is x=&quot;same&quot;, y=&quot;same&quot;
</code></pre>

<p>The <code>makeDisplay</code> function can take this object as is argument for <code>lims</code> and will set panel limits accordingly.</p>

</div>


<div class='tab-pane' id='cognostics'>
<h3>Cognostics</h3>

<p>Much of the power of the viewer for multi-panel displays (particularly when they number in the thousands or higher) lies in the ability to specify metrics that provide interesting information about each panel, with which you can filter and sort your collection of panels to look for those which are interesting.  John Tukey called such metrics &quot;cognostics&quot;.</p>

<p>We can obtain cognostics for our panels by specifying a <code>cogFn</code> function to <code>makeDisplay</code>.  </p>

<p>The cognostics function is applied to each subset just like the plot function and must return a list which can be flattened into a data.frame.  There are helper functions <code>cog()</code>, <code>cogMean()</code>, <code>cog(Range)</code>, etc. which can be used when defining this list.  The most generic, <code>cog()</code> basically wraps the metric you want to compute with additional information, such as the description of the cognostic and its type.  Current types are:</p>

<ul>
<li><code>int</code>: integer </li>
<li><code>num</code>: floating point</li>
<li><code>fac</code>: factor (string)</li>
<li><code>date</code>: date</li>
<li><code>time</code>: datetime</li>
<li><code>geo</code>: geographic (a vector of lat and lon)</li>
<li><code>rel</code>: relation (not implemented)</li>
<li><code>hier</code>: hierarchy (not implemented)</li>
</ul>

<p>If type is not specified, it is inferred based on the data being processed.</p>

<p>In the future, support for input variables will be added (this existed in older versions).  These will not be computed based on the data, but will be placeholders for users to provide panel-specific input.</p>

<p>For our buoy data, there are several congnostics we might be interested in.  Typically the most meaningful cognostics are arrived upon iteratively.  Here, we specify the mean latitude and longitude, the drift (range) of the latitude and longitude, the range of temperature and date variables, the number of observtions, the number of missing values, and an estimate of the variability of the data around a fitted loess line of temperature vs. time.</p>

<pre><code class="r">airDateCogFn &lt;- function(dat) {
   list(
      meanLoc = cog(c(mean(dat$lat), mean(dat$lon)), 
         desc=&quot;Mean lat/lon buoy location&quot;,
         type=&quot;geo&quot;),
      latDrift = cogRange(dat$lat, 
         desc=&quot;Range of latitude drift of the buoy over time&quot;),
      lonDrift = cogRange(dat$lon,
         desc=&quot;Range of longitude drift of the buoy over time&quot;),
      tempRng = cogRange(dat$airTemp, desc=&quot;Temperature range&quot;),
      dateRng = cogRange(as.numeric(dat$date), desc=&quot;Date range (days)&quot;),
      startDate = cog(min(dat$date), 
         desc=&quot;Date of first record&quot;, type=&quot;date&quot;),
      nObs = cog(nrow(dat), desc=&quot;Number of observations&quot;),
      nNA = cog(length(which(is.na(dat$airTemp))),
         desc=&quot;Number of missing observations&quot;),
      locVar = cogLoessRMSE(as.numeric(dat$date), dat$airTemp, degree=2, span=0.1,
         desc=&quot;Variability of temperature around a time-local mean&quot;)
   )
}
</code></pre>

<p>Notice the functions <code>cogMean</code>, <code>cogRange</code>, and <code>cogLoessRMSE</code>.  These are special cognostic functions (and there will be more) to simplify the process of specifying common cognostics.  The difference between <code>cogRange</code> and <code>range</code> and others is that there is removal of NAs and extra checking for errors (when all are NA) so that cognostic calculation is robust to errors.</p>

<p>Note that if you don&#39;t want to wrap your calculations in <code>cog()</code>, you don&#39;t have to, but doing so allows you to control the type of variable and give it a description.</p>

<p>Let&#39;s test it on a subset:</p>

<pre><code class="r">airDateCogFn(buoySplit[[1]])
</code></pre>

<pre><code>    meanLat  meanLon latDrift lonDrift tempRng dateRng nObs nNA    locVar
1 -7.976856 54.99609     1.01     0.78    4.92    2099 2090 470 0.2390464
</code></pre>

<p>We will see how all of these components are used in the next section.</p>

<p>Note: the default behavior for storing cognostics is as an R data.frame.  This provides convenience in computations regarding cognostics in the viewer, but is limiting from a scalability point of view, as it will become difficult to manage with the number of panels exceeding the low millions.  There is now experimental support for storing cognostics in a MongoDB collection.  In this case, it should be possible to handle a much larger number of panels.</p>

<!-- ### Input Variables ### -->

</div>


<div class='tab-pane' id='putting-it-all-together'>
<h3>Putting it All Together</h3>

<p>This may seem like a lot of work for one display over the bare bones example we looked at.  We have done prepanel computation, set axis limits, and specified a cognostics function with cognostic variable descriptions.  This really isn&#39;t very much work for the benefit we receive from what we are able to do with the display after it&#39;s created.  So let&#39;s create it:</p>

<pre><code class="r">makeDisplay(
   group   = &quot;buoy&quot;,
   name    = &quot;airTemp_vs_date&quot;,
   desc    = &quot;Time series plot of air temperature vs. date for each buoy&quot;,
   data    = buoySplit,
   plotFn  = airDatePlotFn, 
   plotDim = list(height=350, width=800),
   lims    = buoyLims, 
   cogFn   = airDateCogFn
)
</code></pre>

<pre><code>Validating &#39;plotFn&#39;...
Validating plot dimensions...
Testing cognostics function on a subset ... ok
Generating plots...
-- Plotting panel 67 of 67 
Computing cognostics...
Updating displayList...
Updating displayList...
Writing panel keys...
Writing cognostics...
Plotting thumbnail...
</code></pre>

<p>Note that in addition to everything else we have done to prepare for the display, we also specify the dimensions of each panel (in pixels) - (perhaps I should change parameters to <code>panelFn</code> and <code>panelDim</code>).  Currently, this is how aspect ratios of the bounding boxes are specified.  There is functionality for you to specify an aspect ratio and either a height or a width, where the bounding box with which the panel aspect ratio is computed is based on the data, not the entire plot (including axis labels, etc.), but it isn&#39;t quite working.  You can specify aspect ratios in your lattice plots, but it is then difficult to get the panel dimensions correct such that there is not a lot of white space wasted for each panel (remember, we want to tile many of these across one screen so we don&#39;t want to waste white space).  On a related note, for lattice plots, see <code>?noMargins</code>.</p>

<p>Now, you can view the display:</p>

<pre><code class="r">view()
# or
view(name=&quot;airTemp_vs_date&quot;)
</code></pre>

<p>We will put off details of the viewer again, although feel free to play around with it.</p>

</div>


<div class='tab-pane' id='another-display'>
<h3>Another Display</h3>

<p>Now that we went through a detailed description of how to create a display, let&#39;s get a little more practice and create another display.  It might be interesting to make a plot for each buoy of how it has moved over time, with the same color indication of whether one date has more than observation.  Since every buoy is at a different latitude and longitude, we already know that we want the x and y axes to be sliced so we will skip the prepanel steps.</p>

<pre><code class="r">buoyMovePlotFn &lt;- function(dat) {
   xyplot(lat ~ lon, groups=duplicated(date) | duplicated(date, fromLast=TRUE), data=dat)
}

makeDisplay(
   group   = &quot;buoy&quot;,
   name    = &quot;lat_vs_lon&quot;,
   data    = buoySplit,
   plotFn  = buoyMovePlotFn,
   lims    = list(x=&quot;sliced&quot;, y=&quot;sliced&quot;)
)
</code></pre>

</div>


<div class='tab-pane' id='single-panel-displays'>
<h3>Single-Panel Displays</h3>

<p>For displays with many panels, it is worth the small amount of extra effort we went through in the previous section to get a nice, viewable display.  However, we are not always doing detailed visualization that requires such displays.  Suppose we have some simple plots we want to add to our VDB.  This can be done by creating a plot object (or expression for base R graphics) and passing it to <code>makeDisplay</code> as the <code>data</code> argument.</p>

<p>For example, suppose that we want to investigate the dependence of the variance of air temperature on the mean for each buoy.  Below is an example of how to create a <code>plotFn</code> for this using the three possible approaches: (1) any base R plot command wrapped in an expression, (2) lattice, (3) ggplot.</p>

<pre><code class="r">means &lt;- sapply(buoySplit, function(x) 
   median(x$airTemp, na.rm=TRUE))
vars &lt;- sapply(buoySplit, function(x) 
   mad(x$airTemp, na.rm=TRUE))

p &lt;- expression(plot(means, vars))  # base R
p &lt;- xyplot(vars ~ means, aspect=1) # lattice
p &lt;- qplot(means, vars, aspect=1)   # ggplot2
p
</code></pre>

<p><center>
   <img src="figs/2plot3.png" width="550px">
</center></p>

<p>Now to add it to the VDB:</p>

<pre><code class="r">makeDisplay(
   group=&quot;buoy&quot;,
   name=&quot;tempVars_vs_tempMeans&quot;,
   desc=&quot;Average variance of temperature at each buoy vs. average mean&quot;,
   data=p
)
</code></pre>

<pre><code>Validating plot dimensions...
Generating plots...
-- Plotting trellis / ggplot / base R plot object.
Updating displayList...
Updating displayList...
Writing panel keys...
Writing cognostics...
Plotting thumbnail...
</code></pre>

</div>


<div class='tab-pane' id='splod'>
<h3>&quot;splod&quot;</h3>

<p>Scatterplot matrices become increasingly infeasible as the number of variables grows beyond \(\approx\)10.  Using <code>trelliscope</code>, we can create &quot;scatterplot displays&quot;, where for \(p\) variables we create a panel for each of the \(\tbinom{p}{2}\) pairwise combinations.  We can leverage research on &quot;scagnostics&quot; (scatterplot diagnostics, or cognostics for scatterplots) <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=leland%20wilkinson%20scagnostics&amp;source=web&amp;cd=1&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.62.6148%26rep%3Drep1%26type%3Dpdf&amp;ei=6spVUcCQDYWl4AOJ_oHoDw&amp;usg=AFQjCNH0rmXCgq5vR-mEkuYz8AC476e6Bw&amp;sig2=VipicfJ0CN0AOJ4pt_yuXg&amp;bvm=bv.44442042,d.dmg&amp;cad=rja">reference</a>, which includes the R package <a href="http://cran.r-project.org/web/packages/scagnostics/">scagnostics</a> to identify interesting relationships.  I have created a convenience function for doing this, <code>splod()</code> (<strong>s</strong>catter<strong>plo</strong>t <strong>d</strong>isplay -- <em>when you have so many variables that <code>splom</code> is going to make your computer <code>splod</code></em>).  This function takes a data.frame and creates a subset for the data for each pairwise combination of variables.  The plot function is a simple scatterplot, and the cognostics function is a collection of scagnostics.</p>

<p>To illustrate this, we will shift to a dataset in a completely different domain, baseball.  In real life there would probably not be a project that included both elnino data and baseball data, but we&#39;ll put this new plot into our existing project.</p>

<pre><code class="r">data(batting)
str(batting)
</code></pre>

<p>There are 24 variables.  The first 5 variables are descriptive and we do not want to include them in our scatterplot display.</p>

<p>Creating a scatterplot display is as simple as</p>

<pre><code class="r">splod(
   batting, 
   id.vars=c(&quot;playerID&quot;, &quot;yearID&quot;, &quot;stint&quot;, &quot;teamID&quot;, &quot;lgID&quot;)
)
</code></pre>

<p>This creates all pairwise groupings of variables not found in <code>id.vars</code> and puts them into a &quot;localDiv&quot; object and passes them to <code>makeDisplay</code>, with the default name being the same as the name of the data passed in concatenated with &quot;_splod&quot;, a default description of &quot;Scatterplot display&quot;, a cognostics function that computes several scagnostics (see <code>cogScagnostics()</code>), and a default plotting function that simply plots one variable vs. another.  You can override these and pass additional parameters to makeDisplay such as a custom cognostics function.</p>

<p>If you want to do something to the data prior to passing it to splod, you can do the above in two steps:</p>

<pre><code class="r">batSplodDat &lt;- makeSplodData(batting, id.vars=c(&quot;playerID&quot;, &quot;yearID&quot;, &quot;stint&quot;, &quot;teamID&quot;, &quot;lgID&quot;))

splod(batSplodDat)
</code></pre>

</div>


<div class='tab-pane' id='shiny-viewer'>
<h3>Shiny Viewer</h3>

<p>The VDB viewer we have encountered throughout this document is a server-side web viewer that uses <a href="http://www.rstudio.com/shiny/">Shiny</a>.  This viewer basically provides a live R session running behind the web viewer, facilitating all of the interaction.  </p>

<p>This viewer is very much a work in progress.  There are many possible features.  Focus is given first to functionality - I will work to optimize things (such as indexing the cognostics data.frame for faster sorting / filtering) once things settle.</p>

<p>Below I describe some of the components of the viewer.</p>

<h4>Panel View</h4>

<p>This is the main component of the viewer.  Once you have selected a display to view, the panel view tiles the panels of the chosen display and allows you to page through them.  Left and right keys or the buttons on the header bar page through panels.  The text box at the top of the window specifies (approximately) how many panels to tile across the screen (exact number is based on the aspect ratio of the panels).</p>

<h4>&quot;View&quot; Modal</h4>

<p>Click the &quot;view&quot; button.  You can specify which cognostics to display below each panel.  The second tab, &quot;plotFn&quot; will let you change the plot function applied to each subset (this only works when <code>storage</code> is &quot;localData&quot;<code>or</code>&quot;rhipeData&quot;`.</p>

<h4>&quot;Cognostics&quot; Modal</h4>

<p>Here, you can sort and filter the panels based on their cognostics.  Click the histograms to get an interactive histogram with which you can specify a range.  On the multivariate selection row, click and drag table cells to specify all variables you want to do multivariate filtering on.  If you choose 2 variables, you are given an interactive scatterplot on which you can select ranges to filter on.  If you select more than 2 variables, you get a bivariate view of a projection pursuit to filter on (actual filtering in this case doesn&#39;t work right now).  I need to give some more thought to making this interface optimal.  The load, save, reset filter settings don&#39;t work right now.  When you leave this modal, you go back to panel view with the panels arranged according to what you did in the cognostics panel.</p>

<h4>Changing the Display</h4>

<p>This is available as first menu item in the display button dropdown.  It simply brings up a modal that lets you select a different display to view.</p>

<h4>Adding a Related Display</h4>

<p>Displays with the same set of keys can have their panels displayed together.  (When a display is created, a hash is made of the sorted unique panel keys.  If two displays have the same hash, it means that they have the exact same collection of panel keys).</p>

<p>As an example, change the display to &quot;airTemp_vs_date&quot;.  Then select &quot;Add Related&quot; from the &quot;Display&quot; button dropdown and select &quot;lat_vs_lon&quot;.  Now you can view the movement of the buoy in association with the time series plots, and filter these to explore whether the lat/lon binning looks valid.</p>

<h4>Keyboard Shortcuts</h4>

<p>There are some keyboard shortcuts for convenience:</p>

<ul>
<li><strong>&quot;c&quot;</strong>: open cognostics modal</li>
<li><strong>&quot;v&quot;</strong>: open view modal</li>
<li><strong>&quot;d&quot;</strong>: open display modal</li>
<li><strong>&quot;r&quot;</strong>: open related displays modal</li>
<li><strong>esc</strong>: leave a modal and return to panel view</li>
</ul>

<h4>To Come</h4>

<ul>
<li>A dropdown for each column (variable) in the cognostics table with options to:

<ul>
<li>apply a transformation to the variable</li>
<li>randomly sample displays to view across the distribution of this variable</li>
<li>if a continuous variable, discretize into bins</li>
</ul></li>
<li>A collapsible sidebar in the panel view that allows you to see where specified cognostics for the panels currently being viewed lie with respect those cognostics of all panels - either in a univariate or bivariate view</li>
<li>Ability to specify panel-to-panel relationships within and between displays through cognostics, specified with something like <code>&quot;group1/name1/key1;group2/name2/key2&quot;</code></li>
<li>Add an interactive network graph using d3 for panel-to-panel relationships, used to select panels of interest</li>
<li>Multi-user per-panel input / user &quot;authentication&quot;</li>
<li>...</li>
</ul>

</div>


<div class='tab-pane' id='cognostics-interactions'>
<h3>Cognostics Interactions</h3>

<p>The current viewer is a proof-of-concept, and I&#39;d like to see much more functionality here in the future.  In particular, the following are interactive filtering / sorting behaviors I would like to see for each cognostics type:</p>

<ul>
<li><strong>Numeric</strong>: filter / expore by univariate quantile plot / histogram - for groups of numeric variables, filter by scatterplot</li>
<li><strong>Factor (Categorical)</strong>: filter / explore by dropdown of categories, frequency bar chart, regular expression input</li>
<li><strong>Date/time</strong>: filter / explore by time window slider, time of day, day of week, calendar range, regular expression input</li>
<li><strong>Geo</strong>: filter / explore by lat/lon projected onto map</li>
<li><strong>Relation</strong>: specified by a list of keys pointing to other subsets (within same dataset or in others) - filter / explore by node groupings in a network graph which might be organized or colored by other cognostics variables</li>
<li><strong>Hierarchy</strong>: a label that is found in a hierarchy specified externally to the record (in a separate metadata file) - filter / explore by treemap, etc.</li>
</ul>

<p>Filters and interactivity based on combinations of cognostics would also be desirable, such as a numeric cognostic plotted versus a time cognostic as a time series, etc.</p>

</div>


<div class='tab-pane' id='client-side-viewer'>
<h3>Client-Side Viewer</h3>

<p>There is a client-side viewer that is convenient because it does not require a web server, but is not nearly as flexible because it requires collections of plot files as the storage mechanism (instead of applying the plot function on the fly or storing the plots on HDFS or in MongoDB).  This viewer can be invoked by calling <code>view(type=&quot;cs&quot;)</code>.  It has not been maintained in a while.</p>

</div>


<div class='tab-pane' id='architecture'>
<h3>Architecture</h3>

<p>So far, all examples have run easily on a local workstation, with no other components necessary.  Trelliscope is designed to scale, but to do so requires a special configuration of hardware and software to be set up.  This can be a difficult process, and I try to provide some good guidelines in this section.  </p>

<!-- For now, installation and setup is geared toward the experienced.  In the future, as things mature, we hope to have a more automated approach to installation. -->

<p>To help understand the architecture that Trelliscope is currently able to support, the figure below shows a diagram of the different components.</p>

<p><center>
   <img src="figs/architecture.png" width="550px">
</center></p>

<p>This is the most complex arrangement.  Each component (R workstation, web server, cognostics server, cluster) is its own entity.  In cases where there is a beefy cluster master node, it is an option to run the R workstation, web server, and cognostics server directly on the namenode.  In this setup, the &quot;R workstation&quot; functionality is attained simply by SSH-ing into the master node and running the R console.  A much better option in this scenario is to install <a href="link">RStudio Server</a> on the master node, giving any analyst a web-based R IDE running directly on the cluster.  Alternatively, RStudio Server can be installed on a separate &quot;R node&quot;.  An upside to this is that all of the configuration for how services communicate can be configured on that machine, so that the analyst doesn&#39;t need to worry about getting configurations set up on their workstation.</p>

<p>In the remainder of this section, I provide installation instructions, guidelines, and references for each of the components, based on a the assumption of RHEL6 being the OS on each machine.  While what is actually required will vary from one OS to another, the general guidelines will hopefully give enough of a blueprint that installation should be doable with a little help from Google.  Given the variability even within the same OS for how things are set up (additional components being installed, firewall settings, etc.), there is no doubt that this will be necessary.  Note that you absolutely must have sudo access on these machines to install things.  I know of no way around this.</p>

<p>The installation and configuration of each of these components is tricky.  However, after dealing with several Hadoop installations and configurations over the years, I don&#39;t feel bad at all about any additional difficulty I have added on top of installing Hadoop itself.  Hadoop has massive enterprise backing and still is a nightmare to install and configure.</p>

</div>


<div class='tab-pane' id='hadoop-r-rhipe'>
<h3>Hadoop, R, RHIPE</h3>

<h4>Install Hadoop:</h4>

<p>There are many references for Hadoop installation and configuration.  This is not the place for details so I will provide some links.  Hopefully you either already have a Hadoop cluster, have EC2, or have someone else who is willing to be your Hadoop expert.</p>

<p>RHIPE is mostly used and tested on the <a href="link">Cloudera</a> distribution.  They provide a GUI installer that you can try.  RHIPE will work with both CDH3 and CDH4 (MRv1).  </p>

<p>Before you move on to the installation of R and RHIPE, run a test Hadoop job to make sure the Hadoop is working.</p>

<p>The following environment variables need to be set so that RHIPE knows how to communicate with Hadoop.  Here as an example I show what these variables are set to for a default CDH4 (MRv1) cluster:</p>

<pre><code>export HADOOP_HOME=/usr/lib/hadoop
export HADOOP_LIBS=/usr/lib/hadoop-0.20-mapreduce:/usr/lib/hadoop-hdfs
export HADOOP_CONF_DIR=/etc/hadoop/conf
export HADOOP_BIN=/usr/lib/hadoop/bin
</code></pre>

<p>These should go in your shell profile.</p>

<h4>Note on installation of R, RHIPE</h4>

<p>The components discussed in the rest of this section (protocol buffers, R, RHIPE) should all be installed on the master node of the Hadoop cluster.  If you have a nice mechanism to share these installations across all machines in the cluster (perhaps a shared drive or directory or a service like 411), you can use that to make the installation available on all nodes.</p>

<p>As an alternative, if your local workstation has the same architecture and OS as the machines in the Hadoop cluster, you can build these components on your local workstation and create a RHIPE binary archive that can be distributed to the nodes using Hadoop&#39;s distributed cache (more on how to do this later).</p>

<h4>Install protocol buffers:</h4>

<p>Must be version 2.4.1.</p>

<p>If you have a specific location you would like it to go, specify <strong>protobuf_prefix</strong>.</p>

<pre><code>wget http://protobuf.googlecode.com/files/protobuf-2.4.1.tar.gz
tar -xzf protobuf-2.4.1.tar.gz
cd protobuf-2.4.1
./configure --prefix=__protobuf_prefix__
make
sudo make install
</code></pre>

<h4>Install R:</h4>

<p>I recommend installing from source.  Need to make sure it installed as a shared library.</p>

<pre><code>### install R
wget http://cran.r-project.org/src/base/R-3/R-3.0.1.tar.gz

sudo mkdir -p /share/apps/R/3.0.1
tar xvfz R-3.0.1.tar.gz
cd R-3.0.1
./configure --enable-R-shlib --prefix=__R_prefix__ --with-readline=no
make
make check
make install
</code></pre>

<p>Install rJava and make sure R / Java is configured properly:</p>

<pre><code># in R
install.packages(&quot;rJava&quot;)
# back out of R:
sudo R CMD javareconf
</code></pre>

<p>This may require ensuring that environment variables such as JAVA_HOME are properly set.</p>

<h4>Install RHIPE:</h4>

<p>To install RHIPE, you need to make sure two environment variables are set.  For future use, these should be added to your shell profile.  I&#39;m using a bash shell here.</p>

<pre><code># install RHIPE
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:__protobuf_prefix__/protobuf/lib/pkgconfig
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:__R_prefix__/lib64/R/lib:__protobuf_prefix__/protobuf/lib
R CMD INSTALL Rhipe_0.73.1.tar.gz
</code></pre>

<h4>Making a runner for RHIPE:</h4>

<p>For RHIPE jobs to run, several environment variables need to be available.  We have discussed these and have instructed you to put them in your shell profile.  Depending on your Hadoop installation, jobs may not be executed by your user, but by a special mapreduce user.  This user will not have these environment variables available, so you need will need to write a special &quot;runner&quot;, a shell script that launches each RHIPE map or reduce task.  Based on my setup, here is what a runner can look like:</p>

<pre><code>#!/bin/bash
export PKG_CONFIG_PATH=$PKG_CONFIG_PATH:/share/apps/protobuf/lib/pkgconfig
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/lib:/usr/local/lib64:/usr/java/jdk1.7.0_03/jre/lib/amd64/server:/share/apps/R/3.0.1//lib64/R/lib:/opt/openmpi/lib:/share/apps/R/3.0.1/lib64/R/lib:/share/apps/protobuf/lib

export HADOOP_HOME=/usr/lib/hadoop
export HADOOP_LIBS=/usr/lib/hadoop-0.20-mapreduce:/usr/lib/hadoop-hdfs
export HADOOP_CONF_DIR=/etc/hadoop/conf
export HADOOP_BIN=/usr/lib/hadoop/bin

export R_HOME=/share/apps/R/3.0.1/lib64/R
export RHIPE_BIN=$R_HOME/library/Rhipe/bin
exec $R_HOME/bin/Rcmd $RHIPE_BIN/RhipeMapReduce --slave --silent --vanilla
</code></pre>

<p>If you save this to a file, say <code>/tmp/rhRunner.sh</code>, then you can specify for RHIPE to use it by setting an option in R (see following example).  You might need to <code>chmod 777</code> this file.</p>

<h4>A RHIPE test job:</h4>

<pre><code class="r">library(Rhipe)
rhinit()
rhoptions(runner=&quot;/tmp/rhRunner.sh&quot;)

permute &lt;- sample(1:150, 150)
splits &lt;- split(permute, rep(1:3, 50))

irisSplit &lt;- lapply(seq_along(splits), function(x) {
   list(x, iris[splits[[x]],])
})

rhwrite(irisSplit, file=&quot;/tmp/irisData&quot;, numfiles=3)

maxMap &lt;- rhmap({
   by(r, r$Species, function(x) {
      rhcollect(
         as.character(x$Species[1]),
         max(x$Sepal.Length)
      )
   })
})

reduce &lt;- rhoptions()$templates$range

res &lt;- rhwatch(map=maxMap, reduce=reduce, input=&quot;/tmp/irisData&quot;, output=&quot;/tmp/irisMax&quot;)
</code></pre>

</div>


<div class='tab-pane' id='other-components'>
<h3>Other Components</h3>

<h4>Web server: node.js</h4>

<p><a href="https://github.com/joyent/node/wiki/Installation">Installation link</a> from node.js.</p>

<p>Here are instructions to build node.js from source.  Note that this requires Python 2.6.  Also note that node versions change frequently, so check the latest version.</p>

<pre><code>wget http://nodejs.org/dist/v0.10.6/node-v0.10.6.tar.gz
tar xzf node-v0.10.6.tar.gz
cd node-v0.10.6
python ./configure
sudo make -j 9
sudo make install
</code></pre>

<h4>Web server: shiny-server</h4>

<p>These instructions are taken from <a href="link">here</a> and <a href="link">here</a>.  Please visit these references for more details.</p>

<pre><code>### Install and configure shiny-server node module
sudo npm install -g shiny-server 
# may need to do sudo /usr/local/bin/npm install -g shiny-server if it is not on sudo&#39;s path

# Create a system account to run Shiny apps
sudo /usr/sbin/useradd -r shiny
# Create a root directory for your website
sudo mkdir -p /var/shiny-server/www
# Create a directory for application logs
sudo mkdir -p /var/shiny-server/log

# loosen up permissions on the app directory
sudo chmod 777 /var/shiny-server/www

# make a configuration file that uses port 81 (change as desired)
sudo mkdir /etc/shiny-server
# as root
cat &lt;&lt;EOF &gt; /etc/shiny-server/shiny-server.conf
run_as shiny;
server {
   listen 81;

   location / {
      site_dir /var/shiny-server/www;
      log_dir /var/shiny-server/log;
      directory_index on;
   }
}
EOF
</code></pre>

<p>Might need to change firewall settings.</p>

<p>Another problem I have had is sudo not having /usr/local/bin on its path.  There are lots of ways around this - this is one of them:</p>

<pre><code>sudo ln -s /usr/local/bin/node /usr/bin/node
sudo ln -s /usr/local/bin/shiny-server /usr/bin/shiny-server
sudo /usr/local/bin/shiny-server
</code></pre>

<p>Running the server:</p>

<p>On RHEL6:</p>

<pre><code># shiny-server.conf
# as root:
cat &lt;&lt;EOF &gt; /etc/init/shiny-server.conf
description &quot;Shiny application server&quot;
start on stopped networking
stop on runlevel [016]
limit nofile 1000000 1000000
exec shiny-server &gt;&gt; /var/log/shiny-server.log 2&gt;&amp;1
respawn
EOF

sudo start shiny-server
</code></pre>

<p>Otherwise can start it manually with:</p>

<pre><code>sudo shiny-server
</code></pre>

<p>The web server also needs R and RHIPE installed if it wants to be able to serve displays that depend on RHIPE data objects.  See the &quot;Local Workstation&quot; for more on this.</p>

<h4>Cognostics server: MongoDB</h4>

<p>This is based on these
<a href="http://docs.mongodb.org/manual/tutorial/install-mongodb-on-red-hat-centos-or-fedora-linux/">installation instructions</a>.  At this link there are also instructions for other OSes.</p>

<pre><code># add repository information
# as root:
cat &lt;&lt;EOF &gt;&gt; /etc/yum.repos.d/10gen.repo
[10gen]
name=10gen Repository
baseurl=http://downloads-distro.mongodb.org/repo/redhat/os/x86_64
gpgcheck=0
enabled=1
EOF

# install
sudo yum install mongo-10gen mongo-10gen-server

# create database directory
sudo mkdir -p /data/db
sudo chown mongo /data/db

# start service
sudo service mongod start
</code></pre>

<h4>Local Workstation</h4>

<p>The local workstation needs R, <code>datadr</code>, <code>Rhipe</code>, and <code>trelliscope</code> installed.  Recall that &quot;local workstation&quot; can simply mean installing these packages on the master node and running R from there.  In this case, we already have R and <code>Rhipe</code> installed and only need to install <code>datadr</code> and <code>trelliscope</code>, which is easy:</p>

<pre><code class="r">library(devtools)
install_github(&quot;datadr&quot;, &quot;hafen&quot;)
install_github(&quot;trelliscope&quot;, &quot;hafen&quot;)
</code></pre>

<p>Installation of RHIPE on a local workstation requires protocol buffers and R installed as a shared library, as outlined above.  You don&#39;t need Hadoop to install RHIPE.  To run RHIPE on your local workstation, however, you need the .jar and configuration files available (a Hadoop cluster does not need to be running).  We will be adding utility functions to RHIPE that will build a bundle of these files on your cluster master node and allow you to move it to your local workstation, simplifying this process.</p>

</div>


<div class='tab-pane' id='vdb-connections'>
<h3>VDB Connections</h3>

<p>Now that all of the components are installed, they need to be able to talk to each other.  Currently, it is simply assumed that each machine can communicate with others through passwordless SSH.  Connections are managed by the <code>vdbConn</code> list of options, available in the <code>conn.R</code> file in your VDB directory.  Here is an example:</p>

<pre><code class="r">options(vdbConn = list(
   ## local machine
   vdbName = &quot;testVDB&quot;,
   vdbPrefix = &quot;~/Desktop/vdbTest&quot;,
   defaultStorage = &quot;local&quot;

   ## hadoop backend
   hadoopConn = list(
      # default HDFS location for vdb plots to go
      hdfsPrefix = &quot;/tmp&quot;,
      # location of a hadoop bundle (see genLocalBundleFiles in Rhipe) for communication with hadoop cluster
      hadoopBundleDir = NULL,
      # an alternative to hadoopBundleDir: specify locations of hadoop directories necessary for communication with cluster
      HADOOP_CONF_DIR = NULL,
      HADOOP_HOME = NULL,
      HADOOP_LIBS = NULL
      # if environment variables are already set up, environment variables and hadoopBundleDir can be NULL
   ),

   ## mongo
   mongoConn = list(
      mongoHost = &quot;localhost&quot;,
      mongoName = &quot;&quot;,
      mongoUser = &quot;&quot;,
      mongoPass = &quot;&quot;
   ),

   ## web server
   webConn = list(
      # IP address of web server (need passwordless ssh to this address)
      ip = &quot;123.45.67.8&quot;,
      # user of 
      user = &quot;someuser&quot;,
      # URL at which shiny-server is broadcasting
      url = &quot;http://something.com&quot;, 
      port = &quot;81&quot;, # port the shiny-server is configured to broadcast on
      # shiny app directory on web server
      appDir = &quot;/var/shiny-server/www&quot;, 
      # where should a copy of your local vdb files go on the web server?
      vdbPrefix = &quot;~/Desktop/vdbTest&quot;
   )
))
</code></pre>

<p>The <code>hadoopConn</code> settings tell a machine (local workstation, web server) how it can talk to the Hadoop cluster.</p>

<p>The <code>mongoConn</code> settings tell a machine (local workstation, web server, cluster) how to communicate with MongoDB.</p>

<p>The <code>webConn</code> settings tell a machine (local workstation, web server) where the web server is and where important components of the web viewer are located.  The <code>url</code> helps Trelliscope know how to construct web URLs to your displays.  The IP and username are supplied so that the local workstation can sync files with the web server.  The <code>vdbPrefix</code> is where a copy of your local VDB directory will reside on the web server.  The <code>appDir</code> directory tells Trelliscope where to put the viewer files for your project.</p>

<p>When you want to update the web server with changes you have made on your local workstation, you can simply call <code>websync()</code>.</p>

</div>


<div class='tab-pane' id='trelliscope-with-rhipe'>
<h3>Trelliscope with RHIPE</h3>

<p>The true inspiration for the <code>trelliscope</code> package was the need to deal with very large datasets.  So far we have looked at in-memory datasets because it helps introduce the different concepts of the package without requiring the user to have a Hadoop cluster at their fingertips.  Here we look at the buoy data in RHIPE.</p>

<p>To start, we need to put the data into RHIPE and have it represented as a RHIPE data object.  This uses the <code>datadr</code> package, which is discussed in great detail <a href="http://hafen.github.com/datadr/">here</a>.  </p>

<p>RHIPE data is stored as key/value pairs, where each pair is a subset of the data split up in some way.  The first step to getting the buoy data into a RHIPE data format on HDFS is to use the <code>rhwrite()</code> function:</p>

<pre><code class="r">library(Rhipe)
rhinit()

hdfs.setwd(&quot;/tmp&quot;)

# write buoySplit data to disk
rhwrite(elnino, file = &quot;elnino&quot;, chunk = 10000, style = &quot;new&quot;)
</code></pre>

<p>Here we set the HDFS working directory to <code>/tmp</code> and write the data.frame into chunks of 10000 rows each.</p>

<p>We can see what this data looks like on HDFS:</p>

<pre><code class="r">rhls(&quot;elnino&quot;)
</code></pre>

<pre><code>  permission   owner      group     size          modtime                 file
1 drwxr-xr-x hafe647 supergroup        0 2013-03-27 01:22 /tmp/elnino/_rh_meta
2 -rw-r--r-- hafe647 supergroup 14221646 2013-03-27 01:22   /tmp/elnino/part_1
</code></pre>

<p>The <code>part_1</code> file holds the data, and the <code>_rh_meta</code> directory is a place for metadata to reside.  Typically large files will be stored across many <code>part_</code> files which are distributed across a multi-node Hadoop cluster, but in this case, the data is very small and we wrote it all to one file.</p>

<p>We can represent this in R as a <strong>&quot;rhDF&quot;</strong> (RHIPE data.frame) object using the following:</p>

<pre><code class="r">library(datadr)
rhElnino &lt;- rhDF(&quot;elnino&quot;)
rhElnino
</code></pre>

<pre><code>An object of class rhDF, rhData with the following attributes: 

&#39;rhData&#39; attr    : value
----------------------------------------------------------------------------
* hasKeys        : [empty] call updateAttributes(dat) to get this value
* loc            : /tmp/elnino
* type           : sequence
* nfile          : 1
* totSize        : 13.56 MB
* ndiv           : [empty] call updateAttributes(dat) to get this value
* splitSizeDistn : [empty] call updateAttributes(dat) to get this value
* sourceData     : [empty] - no source job
* sourceJobData  : [empty] - no source job
* example        : use divExample(dat) or dat$example to get example subset(s)

&#39;rhDF&#39; attr     : value
--------------------------------------------------------------------------------
* vars          : buoy(cha), date(Dat), lat(num), lon(num), zonWinds(num), merWinds(num), humidity(num), airTemp(num), ssTemp(num)
* trans         : identity transformation (original data is a data.frame)
* badSchema     : (not implemented)
* nrow          : [empty] call updateAttributes(dat) to get this value
* splitRowDistn : [empty] call updateAttributes(dat) to get this value
* summary       : (not implemented)
</code></pre>

<p>See the <code>datadr</code> package docs for more details.  With the data represented in this way, we can now apply the buoy division we had done with the local data previously.</p>

<pre><code class="r">byBuoy &lt;- divide(rhElnino, by=&quot;buoy&quot;, output=&quot;byBuoy&quot;)
</code></pre>

<p>The code looks the same as before with the addition of the <code>output</code> argument, which tells where the divided data should reside on HDFS.  Now this data representation, <code>byBuoy</code> behaves pretty much identically to how local data does, which we already know how to handle.</p>

<p>Let&#39;s look at what one subset will look like to our plot function and cognostics function:</p>

<pre><code class="r">str(divExample(byBuoy))
</code></pre>

<pre><code>&#39;data.frame&#39;:   2133 obs. of  9 variables:
 $ buoy    : chr  &quot;buoyid_06&quot; &quot;buoyid_06&quot; &quot;buoyid_06&quot; &quot;buoyid_06&quot; ...
 $ date    : Date, format: &quot;1992-08-21&quot; &quot;1992-08-22&quot; ...
 $ lat     : num  -8.01 -8.01 -8.01 -8 -8 -8.01 -8 -8 -8.01 -8 ...
 $ lon     : num  9.95 9.97 9.97 9.96 9.96 ...
 $ zonWinds: num  -6.5 -6.9 -5.9 -6.8 -7.6 -7.6 -4.3 -3.3 -4.3 -3.5 ...
 $ merWinds: num  -1.1 -1 -1.2 -0.2 2 -0.9 -2.9 -6.4 -3.1 1 ...
 $ humidity: num  78.5 78.1 77 77.3 80.1 77.3 84.8 75.8 73.8 78.1 ...
 $ airTemp : num  NA 29 28.9 28.9 28.9 ...
 $ ssTemp  : num  NA 29.3 29.3 29.4 29.3 ...
</code></pre>

<p>Now you can test the plot and cognostics functions we defined previously:</p>

<pre><code class="r">airDatePlotFn(divExample(byBuoy))
airDateCogFn(divExample(byBuoy))
</code></pre>

<p>And now to plot:</p>

<pre><code class="r">makeDisplay(
   group   = &quot;buoy&quot;,
   name    = &quot;airTemp_vs_date&quot;,
   desc    = &quot;Time series plot of air temperature vs. date for each buoy&quot;,
   data    = byBuoy,
   plotFn  = airDatePlotFn, 
   plotDim = list(height=350, width=800)
)
</code></pre>

<p>This looks just the same as before except that we are now operating on RHIPE data.  The default storage is &quot;hdfs&quot;, meaning the plots will be put on HDFS and read from the with the viewer.  </p>

<h4>hdfsData</h4>

<p>We can also create a display where the actual images are not generated, but instead pointers to the data for each panel are recorded.  Then the viewer can grab the associated data and renders the images on-demand.  An example of this:</p>

<pre><code class="r">makeDisplay(
   group   = &quot;buoy&quot;,
   name    = &quot;airTemp_vs_date&quot;,
   storage = &quot;hdfsData&quot;,
   data    = byBuoy,
   plotFn  = airDatePlotFn, 
   plotDim = list(height=350, width=800)
)
</code></pre>

</div>


<div class='tab-pane' id='panel-storage-types'>
<h3>Panel Storage Types</h3>

<p>I&#39;ve been exploring several storage types for the panels of the displays.  Here is an enumeration with some details.  In general, the main consideration is whether you want to store the raster images on disk or view them on-the-fly.</p>

<p><strong>In favor of storing raster images on disk</strong></p>

<ul>
<li>Paging through panels can be faster as we don&#39;t have to render them prior to displaying them</li>
<li>Less work for the web server</li>
</ul>

<p><strong>In favor of generating images on-the-fly</strong></p>

<ul>
<li><code>makeDisplay()</code> is considerably faster - only need to compute cognostics and metadata about the display.  This is particularly true for very large datasets.</li>
<li>If we are only ever going to view a small subset of possibly hundreds of thousands of panels, why render them all up-front instead of rendering the ones you currently want to look at?</li>
<li>Image resolution: we do now know if the image will be viewed as a single large panel in the window, or tiled with tens or hundreds of panels in a single window.  When we render the image on-the-fly, we can render it at the resolution at which it will be viewed instead of having to store a raster image that has a resolution sufficient to be viewed at the largest scale.  A way around this would be to go away from .png to a vector format like .svg.  The only problem with that is that those files can get very large, and can be difficult to render consistently across all browsers.  In the future, this may all move toward d3...</li>
</ul>

<!-- Another option that might be explored is storing the data for the plots in a way that some .json representation of it can be sent to the browser and rendered very quickly using d3.js... -->

<p>Here is a list of storage types:</p>

<h4>local</h4>

<p>This is what we&#39;ve been dealing with for the most part and is the default for an input of class &quot;localDiv&quot;.  A raster .png image for each panel is stored to local disk in the &quot;displays&quot; directory.</p>

<h4>hdfs</h4>

<p>This is the default when the input is of class <strong>&quot;rhDiv&quot;</strong>.  Base64 encoded raster .png images are stored in a mapfile on HDFS.</p>

<h4>hbase</h4>

<p><em>(Coming soon)</em>.  Base64 encoded raster .png images are stored in an HBASE table.</p>

<h4>mongo</h4>

<p>Base64 encoded raster .png images are stored in MongoDB (connection parameters specified in <code>vdbConn</code>).  This is very experimental and not recommended.</p>

<h4>localData</h4>

<p>The data object of class <strong>&quot;localDiv&quot;</strong> is stored in the vdb directory and the viewer reads the data directly from this object and applies <code>plotFn</code> on-the-fly.  This allows you to change the plot function on-the-fly as well.</p>

<p>Here is an example:</p>

<pre><code class="r">makeDisplay(
   group   = &quot;buoy&quot;,
   name    = &quot;airTemp_vs_date_ld&quot;,
   desc    = &quot;Time series plot of air temperature vs. date for each buoy&quot;,
   storage = &quot;localData&quot;,
   data    = buoySplit,
   plotFn  = airDatePlotFn, 
   plotDim = list(height=350, width=800),
   lims    = buoyLims, 
   cogFn   = airDateCogFn
)
</code></pre>

<p>Navigate to the &quot;View&quot; modal and click the &quot;plotFn&quot; tab.  Now try changing the text of the plot to this, for example, to add a loess smooth:</p>

<pre><code class="r">airDatePlotFn &lt;- function(dat) {
   xyplot(airTemp ~ date, 
      groups=duplicated(date) | duplicated(date, fromLast=TRUE), 
      panel=function(x, y, ...) {
         panel.xyplot(x, y, ...)
         panel.loess(x, y, degree=2, span=0.1, 
            evaluation=300, col=&quot;black&quot;))
      },
      data=dat$data
   )
}
</code></pre>

<h4>hdfsData</h4>

<p>This will allows the same thing to be done with <strong>&#39;rhDiv&#39;</strong> data as with <strong>&#39;localDiv&#39;</strong> - data is rapidly retrieved based on its key and plots are rendered on-the-fly.</p>

<p>Of all of these storage options, the client side viewer can only deal with &quot;local&quot;.  All others require the Shiny viewer.</p>

<h4>Enumeration of input/output options</h4>

<p>As you have seen, I&#39;ve implemented support for a lot of input/output types.  Part of this is to be flexible.  Part is because I&#39;m trying to evaluate what is the best way to go.  The two input formats are objects of class <strong>&#39;localDiv&#39;</strong> and <strong>&#39;rhDiv&#39;</strong>.  The output (&quot;storage&quot;) formats for a given input format are:</p>

<p><strong>Output possibilities for input type &#39;localDiv&#39;</strong>:</p>

<ul>
<li>local <em>(default)</em></li>
<li>localData</li>
<li>mongodb</li>
</ul>

<p><strong>Output possibilities for input type &#39;rhDiv&#39;</strong>:</p>

<ul>
<li>hdfs <em>(default)</em></li>
<li>local (not a good idea unless you have a common file system mounted on all nodes - like LUSTRE)</li>
<li>mongodb</li>
<li>hdfsData</li>
<li>hbase <em>(not implemented)</em></li>
</ul>

</div>


<div class='tab-pane' id='cognostics-storage-types'>
<h3>Cognostics Storage Types</h3>

<p>By default, cognostics are stored as an R data.frame.  This becomes limiting when the number of panels gets into the low millions, as the viewer must read the entire cognostics data.frame in and manipulate it.  </p>

<p>An alternative for storing cognostics that has been experimentally implemented is storing the cognostics as an indexed MongoDB collection.  This should provide a greater deal scalability, but we lose some flexibility of manipulating the entire cognostics data in R.  We will explore how to get around these limitations.  Another downside of using MongoDB for storing cognostics is that it requires one more component to be installed and configured.</p>

<h4>MongoDB Example</h4>

<p>To plot to MongoDB, you first need to set some values in your <code>vdbConn</code>:</p>

<pre><code class="r">options(vdbConn = list(
   vdbName        = &quot;vdbTest&quot;, 
   vdbPrefix      = vdbDir,
   mongoHost = &quot;127.0.0.1&quot;,
   mongoName = &quot;&quot;,
   mongoUser = &quot;&quot;,
   mongoPass = &quot;&quot;
))
</code></pre>

<p>The last for parameters provides information on how to connect to a MongoDB running on a local host (with no username and password).  The database in which collections are stored is provided by <code>vdbName</code>.</p>

<p>Now, going back to our previous example, supposing you have the data, plot, and cognostics functions already in your session, here is how you can write the cognostics to MongoDB:</p>

<pre><code class="r">makeDisplay(
   group   = &quot;buoy&quot;,
   name    = &quot;airTemp_vs_date_mongo&quot;,
   desc    = &quot;Time series plot of air temperature vs. date for each buoy&quot;,
   data    = buoySplit,
   plotFn  = airDatePlotFn, 
   plotDim = list(height=350, width=800),
   lims    = buoyLims, 
   cogFn   = airDateCogFn,
   storage = &quot;mongo&quot;,
   cogStorage = &quot;mongo&quot;
)
</code></pre>

<p>Here we also store the plots in MongoDB, although this is not necessary.  It is possible to store plots in any of the storage options while still writing cognostics to MongoDB.  In the case of RHIPE plot storage, note that all nodes on the Hadoop cluster need to be able to communicate with the MongoDB service.</p>

<p>The above code created two collections in the MongoDB database, &quot;vdbTest&quot;.  These are:</p>

<ul>
<li><strong>buoy_airTemp_vs_date_mongo_cog</strong>: each document in this collection is a row of the cognostics table</li>
<li><strong>buoy_airTemp_vs_date_mongo_panel</strong>: each document is a base64-encoded png.  The <code>_id</code> of this table is the panel key, and ties to &quot;panelKey&quot; in the _cog collection.</li>
</ul>

<p>The prefix &quot;buoy_&quot; in the collection names comes from the &quot;group&quot; argument to makeDisplay.</p>

<p>You can try the viewer on this data.  You will notice that currently the interactive graphical filters do not work with cognostics stored in MongoDB.</p>

<pre><code class="r">view()
</code></pre>

</div>


<div class='tab-pane' id='creating-a-notebook'>
<h3>Creating a Notebook</h3>

<p>When collaborating with others or keeping track of your own research, it is not usually natural to have a bunch of displays lying around.  It is useful to organize them.  This package contains functions that help set up and maintain a web notebook that ties in to the vdb.</p>

<p>Notebooks are static pages, but are dynamically created using the <a href="http://yihui.name/knitr/">knitr</a> and <a href="http://cran.r-project.org/web/packages/markdown/index.html">markdown</a> packages.  I&#39;m not so concerned here about the &quot;reproducible research&quot; facets of knitr the package (difficult to deal with when your code depends on various backend systems that aren&#39;t always available or code that takes a very long time to run), but they make it easier to set the notebooks up and are available if you want to use them.  Mainly, the idea is that you can write a simple notebook using the simple <a href="http://daringfireball.net/projects/markdown/">markdown syntax</a> and not have to focus on layout, etc.  This of course comes with some lack of flexibility - a common tradeoff.</p>

<p>To create a new page, you can do the following:</p>

<pre><code class="r">newNotebook(
   title=&quot;El Nino Buoy Data Exploratory Analysis&quot;, 
   author=&quot;Bob&quot;
)
</code></pre>

<p>There is also an argument, <code>name</code>, which determines the name of the <code>.Rmd</code> file generated.  The default is <code>&quot;index&quot;</code>, so that the new notebook page we set up has the name &quot;index.Rmd&quot; with the specified title and author.  If you recall from earlier, there is a <code>notebook</code> subdirectory in your VDB directory.  This is where the notebook files go.</p>

<p>Let&#39;s look at the first few lines of the file we just created:</p>

<pre><code class="r">vdbDir &lt;- getOption(&quot;vdbConn&quot;)$vdbPrefix
list.files(file.path(vdbDir, &quot;notebook&quot;))
cat(paste(readLines(file.path(vdbDir, &quot;notebook&quot;, &quot;index.Rmd&quot;))[1:10], collapse=&quot;\n&quot;))
</code></pre>

<pre><code>```{r, echo=FALSE}
# do not remove this block - set parameters accordingly
bsSetup(
   pageTitle = &quot;El Nino Buoy Data Exploratory Analysis&quot;, 
   title     = &quot;El Nino Buoy Data Exploratory Analysis&quot;,
   author    = &quot;Bob&quot;,
   toc       = &quot;TRUE&quot;,
   css       = NULL
)
</code></pre>

<p>We see that there is a knitr R code block that gets executed.  This sets up the skeleton of the page using a function <code>bsSetup()</code>.  You can change the options here - (see the function reference for more details).  This sets up a skeleton of a page with a header, optional sidebar with table of contents, etc.  Below this first code chunk in the .Rmd file is where you put your notebook text.  By default, the <code>newNotebook()</code> function creates some default content here that illustrates various aspects of markdown.</p>

<p>Go ahead and open this file in your favorite editor and try pasting in the following lines:</p>

<pre><code class="r"># A look at time series plots by buoy

Some text...

nbDisplay(&quot;tempVars_vs_tempMeans&quot;)

Some text...

nbDisplayList(c(&quot;airTemp_vs_date&quot;, &quot;airTemp_vs_date_ld&quot;))
</code></pre>

<p>The function <code>nbDisplay()</code> is a convenience function for embedding a display into your notebook.  It is typically good for single-panel displays.  The <code>nbDisplayList()</code> function takes a list of display names and creates a block of HTML that provides the thumbnail, name, and description about the display along with links to viewing the display on in the viewer.  </p>

<p>Now typeset it and view:</p>

<pre><code class="r">typeset() # or typeset(name=&quot;index&quot;)
viewNotebook() # or viewNotebook(name=&quot;index&quot;)
</code></pre>

<p>Note that &quot;index&quot; is the default page name but you can make pages with other names.</p>

</div>


<div class='tab-pane' id='syncing-files-with-the-web'>
<h3>Syncing Files With the Web</h3>

<p>This package tries to make it as easy as possible to sync files with the web so that changes to a VDB or notebook can easily be pushed out to everyone.  It is done by simply setting your VDB connection to point to the web server and call <code>websync()</code>.  More on this to come...</p>

</div>

   
   
   <ul class="pager">
      <li><a href="#" id="previous">&larr; Previous</a></li> 
      <li><a href="#" id="next">Next &rarr;</a></li> 
   </ul>
</div>


</div>
</div>

<hr>

<div class="footer">
   <p>&copy; Ryan Hafen, 2013</p>
</div>
</div> <!-- /container -->

<script src="js/jquery.js"></script>
<script src="bootstrap/js/bootstrap.js"></script>
<script src="js/jquery.ba-hashchange.min.js"></script>
<script>
function manageNextPrev() {
   $('a#next').parent().toggleClass('disabled', $('#toc.nav li.active').nextAll('li:not(.nav-header)').size() == 0);
   $('a#previous').parent().toggleClass('disabled', $('#toc.nav li.active').prevAll('li:not(.nav-header)').size() == 0);
}
manageNextPrev();

$('a#next').click(function(e) {
   e.preventDefault();
   location.href = $('#toc.nav li.active').nextAll('li:not(.nav-header)').first().find('a').attr('href');
   manageNextPrev();
});
$('a#previous').click(function(e) {
   e.preventDefault();
   location.href = $('#toc.nav li.active').prevAll('li:not(.nav-header)').first().find('a').attr('href');
   manageNextPrev();
});

$(window).hashchange(function() {
  $('.tab-pane').hide();
  var tab = location.hash || '#background';
  $(tab + '.tab-pane').show();

  $('#toc.nav li.active').removeClass('active');
  $('#toc.nav li a[href="' + tab + '"]').parent().addClass('active');
  manageNextPrev();
});
$(window).hashchange();
</script>
</body>
</html>