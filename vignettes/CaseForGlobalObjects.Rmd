---
title: "Global Objects In Trelliscope VDBs"
author: "Jeremiah Rounds"
date: "November 19, 2015"
output: html_document
---

```{r setup, include=FALSE}
if(FALSE){
	library(rmarkdown)
	rmarkdown::render("../vignettes/CaseForGlobalObjects.Rmd")
}
knitr::opts_chunk$set(echo = TRUE)
```

# The Case For Global Objects

This document assumes familiarity and previous experience with the Trelliscope package.   I have come to the conclusion that some use-cases with Trelliscope would be simpler if there was the concept of a "global R object" within a vdb that was available in all displays/panels.  Here I lay out the case for such a thing using examples from the Trelliscope tutorial.  Though, I admit that global objects are a little against the core design of Trelliscope which was very much aimed at distributed computing and "for each" paradigms in its initial design, and almost everything I argue for could be solved with several well-coordinated and organized `localDiskConn` DDOs.

## Case 1: Doubling The Tutorial Example -- A Case For localDiskConn

Recall an example display was built in the Trelliscope tutorial with
```{r, echo=TRUE, message=FALSE}
library(trelliscope)
library(housingData)
data(housing)
# divide housing data by county and state
byCounty <- divide(housing, by = c("county", "state"))
bareBonesPanel <- function(x)
   plot(x$time, x$medListPriceSqft)
tmp = tempfile()
conn = vdbConn(tmp, "tmp",autoYes=TRUE)
ret = makeDisplay(byCounty,
   panelFn = bareBonesPanel,
   name    = "list_vs_time_barebones",
   desc    = "List price per square foot vs. time") 
```

```{r}
cat(system(paste("du -sh", tmp), intern=TRUE))  #mac or linux like only
```

My vdb function used 5.7M of disk space (there is approximately 2M of space used in an empty vdb),  so now suppose I have two panels?
```{r, echo=TRUE, message=FALSE}
ret = makeDisplay(byCounty,
   panelFn = bareBonesPanel,
   name    = "panel 2",
   desc    = "List price per square foot vs. time",
	conn = conn)
```
```{r}
cat(system(paste("du -sh", tmp), intern=TRUE))  #mac or linux like only
```
I am now up to 7.8M of disk space, but I didn't really add any data to the display database (except I did because it gets duplicated for each panel function).  Suppose instead `housing` was 100 times as large, and I repeat this experiment.  For example,
```{r, message=FALSE}
library(data.table)
housing100 = lapply(1:100, function(i){
		x = housing
		x$medListPriceSqft = jitter(x$medListPriceSqft)
		x$medSoldPriceSqft = jitter(x$medSoldPriceSqft)
		x}) %>% rbindlist %>% as.data.frame
byCounty <- divide(housing100, by = c("county", "state"))
tmp = tempfile()
conn = vdbConn(tmp, "tmp",autoYes=TRUE)
ret = makeDisplay(byCounty,
   panelFn = bareBonesPanel,
   name    = "list_vs_time_barebones",
   desc    = "List price per square foot vs. time",
   conn = conn)
```
```{r}
cat(system(paste("du -sh", tmp), intern=TRUE))  #mac or linux like only
```
And now if I double the number displays?
```{r, message=FALSE}
ret = makeDisplay(byCounty,
   panelFn = bareBonesPanel,
   name    = "panel_2",
   desc    = "List price per square foot vs. time",
   conn = conn) 
```
```{r}
cat(system(paste("du -sh", tmp), intern=TRUE))  #mac or linux like only
```
Doubling the displays doubled the disk-usage of the vdb, so what to do?  It is clearly unnecessary because the source data is identical.  Well one solution (by design) is not to use `kvMemory` for a datadr object. `kvMemory` induces an inefficient copying strategy by Trelliscope.  For example, suppose instead I had chosen to use a `DDO` with a `localDiskConn`.

For example, 
```{r, message=FALSE}
byCounty = localDiskConn(tempfile(), autoYes=TRUE)
byCounty <- divide(housing100, by = c("county", "state"), output=byCounty)
tmp = tempfile()
conn = vdbConn(tmp, "tmp",autoYes=TRUE)
ret = makeDisplay(byCounty,
   panelFn = bareBonesPanel,
   name    = "list_vs_time_barebones",
   desc    = "List price per square foot vs. time",
   conn = conn) 
```
```{r}
cat(system(paste("du -sh", tmp), intern=TRUE))  #mac or linux like only
```
The vdb is nearly the same size as an empty vdb, and doubling the displays does nothing in particular:
```{r, message=FALSE}
ret = makeDisplay(byCounty,
   panelFn = bareBonesPanel,
   name    = "panel_2",
   desc    = "List price per square foot vs. time",
   conn = conn) 
```
```{r}
cat(system(paste("du -sh", tmp), intern=TRUE))  #mac or linux like only
```

This example is a case for `localDiskConn` in any serious professional use of Trelliscope... except... (case 2 and 3).



## Case 2:  What if panelFn needs meta data?  Really _large_ meta data...

Suppose now that panelFn needs external non-divided data in order to correctly render a display.  This comes up in genomic and proteomic experiments.  There are 20k genes in humans.  Each gene may have a quantity observed for N subjects .  Each subject then may have a vector of phenotype information (which constitutes a data.frame of meta information), and then each panelFn needs access to the meta-data to make a correct display.  That introduces the idea of meta-data, and ordinarily that would be handled by auto-detected `params` in `makeDisplay`, and each `panelFn` would get its own copy of meta-data.  Which seems fine, but now suppose meta data is large... very large... as large as the data set itself.    Now what?

For example (but rather cartoonish and contrived)
```{r,eval=FALSE}
meta = housing100 #something large
weirdPanelFunction = function(v){  
	mean = mean(meta$medListPriceSqft, na.rm=TRUE)  #something standing in for a meta-data calculation based on contents of v
	plot(x$time, x$medListPriceSqft)
}
```


In my case in developing proteomics visualizations I was generating a lot of different large meta analytic data products necessary to create different panelFn, and the correct way to handle that in Trelliscope would be to have  its own `localDiskConn` DDO object and have that be the `params` that is detected in `makeDisplay` (as illustrated in Case 1), but then all these external DDO objects need to be coordinated with anyone that wants to use the vdb.  I really just wanted a vdb that collaborators could `view()` without me explaining a bunch about where data resided. The most likely spot to put them would be in the `vdb/data` directory; however, it should be noted that `localDiskConn` does not allow you to specify a _relative_ path, and because of that any auto-detect `params` referring to a `vdb/data` would not tolerate cutting and pasting that vdb into a new directory.  That problem itself might be solved by having a panelFn try to detect if a valid `localDiskConn` is available and if not fix it to be relative to the current working directory (which is the vdb directory), but this is complicated and just placing a DDO into `vdb/data` is not necessarily avoiding the problem of collaboration. In addition to the extra complication in collaboration,  there is the extra disk access cost that comes with using `localDiskConn` for meta-analytic data products.

The idea of a `localDiskConn` DDO for every type of meta-analytic data for a panelFn  is good, but lacks agility, comes with organizational over-head, and an alternative solution is to let Trelliscope displays use global objects that are like `params` except managed by the user and global to all panelFns.


## Case 3: What if you need to deploy to shinyapps.io and you have complicated package code?


Deploying Trelliscope to shinyapps.io requires that your `panelFn` arguments have no dependence on packages other than those on github or CRAN.  In my case, I had decided to build my work into an R package for proteomics.  If I don't want to also deploy that package publicly,  this essentially means that I need in someway to hack my panelFn dependencies into Trelliscope.  The problem with just sourcing everything into R memory is the auto-detect for the params argument in `makeDisplay` does not auto-detect nested function calls.  This implies that any collaborators for my vdb need to be aware of the extra R code they need to source. It would be better to have a "source" directory in a VDB that was automatically handled by Trelliscope.  I mention this as "Case 3" because while working with the code to build global objects into Trelliscope, I also built in sourcing R functions. 



# Solution: global.R and How It Works With Trelliscope.

Trelliscope is a Shiny application.  Shiny itself loads "global.R" from the application directory (See http://shiny.rstudio.com/articles/scoping.html).  Not only is using a Shiny global.R file an easy way to address cases 2 through 3 above, but if a user just places their own global.R file into a vdb then Trelliscope cannot stop it from being loaded.  That said, I provide a couple functions to get going with a provided `global.R` for Trelliscope and adjusted Trelliscope to keep `global.R` synced with the most recent viewer code.  


## New Trelliscope Functions

There are three new Trelliscope functions in R these are
```{r, eval=FALSE}
?globalsFile(conn)
?globalsExist(conn)
?copyRSource(dir, conn)
```

`globalsFile` returns the correct path to an `Rdata` object in which to `save` global data such that it will be carried with and saved by the display.  `globalsExist` returns TRUE if the global data file already exist, and `saveRSource` copies R files from `dir` into a spot within the vdb such that they will be sourced before an panelFn executes.


In addition there is a new argument  `makeDisplay(..., detect.globals=TRUE)`.  When `detect.globals=FALSE` the automatic detection of `params` and `packages` in `makeDisplay` is disabled. Note: this often means you need to supply an extensive `packages` argument such that all the packages your panel functions depend on are specified.


In addition to these changes the `copyViewerFiles` code has been modified to save a new `global.R` as part of the Trelliscope viewer code.


## Examples: Saving Globals

Here is a really simple example:
```{r, message=FALSE, eval=FALSE}
meta = list()
meta$color = "blue"
data(iris)
div = divide(iris, by="Species")
panelFn = function(v) plot(v$Sepal.Length, v$Sepal.Width, col = meta$color)
tmp = tempfile()
conn = vdbConn(tmp, autoYes=TRUE)
ret = makeDisplay(div, name ="tmp", panelFn=panelFn, detect.globals=FALSE)
rm("meta")
```
```{r, eval=FALSE}
view(conn=conn)  
```
The view doesn't work because meta doesn't exist and we disabled the saving of params with `detect.globals=FALSE`, but we can save the meta objects and the vdb will work again:

```{r, eval=FALSE}
meta = list()
meta$color = "blue"
save(meta, file =globalsFile(conn))
rm("meta")
```
```{r, eval=FALSE}
view(conn=conn)
```
We know the global.R ran because we will see `Loading data/globals.Rdata`.
This was a small example, but it has unlimited potential.  The user is unlocked to put anything meaningful in an Rdata object.


## Examples: Copying R Source Files

Suppose instead of global objects I have source files that I want to run before any panel functions. In that case I should use the `copyRSource` function.  Any files ending in `.R` from that directory will be copied to the vdb and sourced when it is loaded. 

For example
```{r, eval=FALSE}
rm("meta")
source_dir = tempfile()
dir.create(source_dir)
cat("meta = list(color = 'blue')\n", file =  file.path(source_dir, "my_source.R"))
source(file.path(source_dir, "my_source.R"))
meta
```
The source file makes the `meta` object, but this R source can do whatever the user wants.  To utilize it we proceed
```{r, message=FALSE, eval=FALSE}
tmp = tempfile()
conn = vdbConn(tmp, autoYes=TRUE)
copyRSource(source_dir, conn=conn)
makeDisplay(div, name ="tmp", panelFn=panelFn, detect.globals=FALSE) 
rm("meta")
```
```{r, eval=FALSE}
view(conn=conn)
```
This time `view()` works because of this automation `Sourcing data/R
data/R/my_source.R`.



# Conclusion

Between `save(..., file=globalsFile(conn))` and `copyRSource` users have unlimited flexibility in customizing the display environment, but in a way that requires very little communication/organization with future collaborators.   












